{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terrain Classification\n",
    "### Created by Keenan McConkey 2019.5.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pymrmr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b36479751495>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdecimal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecimal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpymrmr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pymrmr'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "import sklearn.preprocessing as pre\n",
    "from datetime import datetime\n",
    "from decimal import Decimal\n",
    "\n",
    "import pymrmr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Importing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find .csv files\n",
    "raw_datasets = {}\n",
    "dataset_paths = glob.glob('imu_data/*.csv')\n",
    "N_DATASETS = len(dataset_paths)\n",
    "\n",
    "dataset_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Set indices of a dataframe depending on given label'''\n",
    "def set_indices(dataframe):\n",
    "    if 'Frequency' in dataframe.columns:\n",
    "        return dataframe.set_index('Frequency')\n",
    "    elif 'Epoch Time' in dataframe.columns:\n",
    "        return dataframe.set_index('Epoch Time')\n",
    "    else:\n",
    "        raise('Unknown dataframe columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terrains = ['Concrete', 'Carpet', 'Linoleum', 'Asphalt', 'Sidewalk', 'Grass', 'Gravel']\n",
    "\n",
    "'''Get the terrain type given dataset's label (e.g. WheelLeftConcrete)'''\n",
    "def get_terrain(_label):\n",
    "    for terrain in terrains:\n",
    "        if terrain in _label:\n",
    "            return terrain\n",
    "        \n",
    "    raise Exception('Unknown terrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame_columns = ['X Accel', 'Y Accel', 'Z Accel', 'X Gyro', 'Y Gyro', 'Z Gyro', 'Epoch Time']\n",
    "\n",
    "wheel_columns = ['X Accel', 'Y Accel', 'Z Accel', 'X Gyro', 'Y Gyro', 'Z Gyro', 'Run Time', 'Epoch Time']\n",
    "\n",
    "phone_columns = ['X Accel', 'Y Accel', 'Z Accel', 'X Gyro', 'Y Gyro', 'Z Gyro', 'Run Time', 'Epoch Time']\n",
    "\n",
    "data_columns = ['X Accel', 'Y Accel', 'Z Accel', 'X Gyro', 'Y Gyro', 'Z Gyro']\n",
    "\n",
    "'''Get columns for given label'''\n",
    "def get_columns(_label):\n",
    "    columns = []\n",
    "    \n",
    "    # Columns differ depending on device used\n",
    "    if 'Phone' in _label:\n",
    "        columns = phone_columns.copy()\n",
    "    elif 'Frame' in _label:\n",
    "        columns = frame_columns.copy()\n",
    "    elif 'Wheel' in _label:\n",
    "        columns = wheel_columns.copy()\n",
    "    else:\n",
    "        raise Exception('Unknown label')\n",
    "    \n",
    "    # Transformed datasets replace time columns with frequency\n",
    "    if ('FFT' in _label or 'PSD' in _label):\n",
    "        columns.remove('Epoch Time')\n",
    "        \n",
    "        if 'Wheel' in _label or 'Phone' in _label:\n",
    "            columns.remove('Run Time')\n",
    "        \n",
    "        columns.append('Frequency')\n",
    "    \n",
    "    return columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) - Parsing Data into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import datasets as an array of Pandas DataFrames\n",
    "TRIM_LEN = 2000 # Number of data points to trim from each side\n",
    "N_DATA_COL = 6 # Number of columns containing directional data\n",
    "\n",
    "# Columns not needed right now\n",
    "remove_columns = ['LINEAR ACCELERATION X (m/s²)', 'LINEAR ACCELERATION Y (m/s²)', 'LINEAR ACCELERATION Z (m/s²)',\n",
    "                  'GRAVITY X (m/s²)', 'GRAVITY Y (m/s²)', 'GRAVITY Z (m/s²)', \n",
    "                  'MAGNETIC FIELD X (μT)', 'MAGNETIC FIELD Y (μT)', 'MAGNETIC FIELD Z (μT)',\n",
    "                  'ORIENTATION X (pitch °)', 'ORIENTATION Y (roll °)', 'ORIENTATION Z (azimuth °)']\n",
    "\n",
    "dataset_labels = []\n",
    "\n",
    "for dataset_path in dataset_paths:\n",
    "    # Parse labels from filenames\n",
    "    dataset_label = dataset_path.split(' ')[-1].split('/')[-1].split('.')[0]\n",
    "    dataset_labels.append(dataset_label)\n",
    "    \n",
    "    # Read from CSV to Pandas\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "        \n",
    "    # Drop unused columns\n",
    "    for to_remove in remove_columns:\n",
    "        if to_remove in dataset.columns:\n",
    "            dataset = dataset.drop(to_remove, axis=1)\n",
    "    \n",
    "    # Rename columns to something easier to work with\n",
    "    dataset.columns = get_columns(dataset_label)\n",
    "    \n",
    "    # Trim edges to account for startup time\n",
    "    ## Not necessary if we can remove gravity when IMUs are stationary\n",
    "    ## i.e. Make stationary IMU have zero contribution to FFT\n",
    "    #dataset = dataset[TRIM_LEN:-TRIM_LEN]\n",
    "    \n",
    "    raw_datasets.update({dataset_label: dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check dataset validity for Frame data\n",
    "raw_datasets['FrameMiddleCarpet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check dataset validity for Wheel data\n",
    "raw_datasets['WheelLeftConcrete'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the update rate of the IMU is non deterministic and lower than the rate the phone samples it at, i.e. the phone receives a non-deterministic number of sequential identical measurements from the IMU when polling at approx 200 Hz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) - Visualizing Time Domain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Compare time domain data from phones mounted in different locations'''\n",
    "def phone_compare(dirn, surface):\n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(30, 10))\n",
    "    \n",
    "    # Use epoch times to align data\n",
    "    for mount in ('Left', 'Right', 'Middle'):\n",
    "        plt.plot(raw_datasets['Phone' + mount + surface]['Epoch Time'],\n",
    "                 raw_datasets['Phone' + mount + surface][dirn], label=mount)\n",
    "    \n",
    "    plt.xlabel('Epoch Time (s)')\n",
    "    plt.ylabel(dirn + ' on ' + surface)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare some phone data (some phone datasets are missing!)\n",
    "phone_compare('Z Gyro', 'Linoleum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Compare two Pandas DataFrames by Epoch Time'''\n",
    "def dataset_compare(dataset1, label1, dataset2, label2, dirn, t_offset=0, y_offset=0):\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(figsize=(30,10))\n",
    "    \n",
    "    # Plot data with given y and t offsetson first dataset\n",
    "    ax.plot(dataset1[label1]['Epoch Time'].apply(lambda t: t + t_offset), \n",
    "            dataset1[label1][dirn].apply(lambda y: y + y_offset), label=label1)\n",
    "    ax.plot(dataset2[label2]['Epoch Time'], \n",
    "            dataset2[label2][dirn], label=label2)\n",
    "    \n",
    "    ax.set_xlabel('Epoch Time ($s$)')\n",
    "    ax.set_ylabel(dirn)\n",
    "    ax.set_title(dirn + ' for ' + label1 + ' and ' + label2)\n",
    "    \n",
    "    # Plot offset info\n",
    "    offset_text = 'Offsets\\n'\n",
    "    offset_text += label1 + ': t={}'.format(t_offset) + ', ' + 'y={}'.format(y_offset)\n",
    "    ax.text(0.05, 0.05, s=offset_text, \n",
    "            horizontalalignment='left', verticalalignment='bottom', transform=ax.transAxes)\n",
    "    \n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compare time-corrected phone and frame data on Linoleum\n",
    "dataset_compare(raw_datasets, 'PhoneMiddleLinoleum',\n",
    "                raw_datasets, 'FrameMiddleLinoleum', 'Z Gyro', t_offset=-4.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Plot given x and y axes for every Pandas DataFrame in given array of datasets'''\n",
    "def plot_all(_datasets, x_axis, y_axis, windowed=False, win_num=0):\n",
    "    plt.clf()\n",
    "    \n",
    "    n_axes = len(_datasets)\n",
    "    odd_axes = n_axes % 2 == 1\n",
    "    \n",
    "    rows = int((n_axes + 1) / 2)\n",
    "    \n",
    "    # Scale approriately\n",
    "    if (odd_axes):\n",
    "        fig = plt.figure(figsize=(n_axes*5, n_axes*3))          \n",
    "    else:\n",
    "        fig = plt.figure(figsize=(n_axes*5, n_axes*2))\n",
    "    \n",
    "    # Grid of subplots\n",
    "    gs = gridspec.GridSpec(rows, 2)\n",
    "        \n",
    "    axes = []\n",
    "    row, col = 0, 0\n",
    "    \n",
    "    for i, (label, dataset) in enumerate(_datasets.items()):\n",
    "        # Take a whole row if odd num of axes\n",
    "        if (i == n_axes-1 and odd_axes): \n",
    "            axes.append(fig.add_subplot(gs[row, :]))\n",
    "        else:\n",
    "            axes.append(fig.add_subplot(gs[row, col]))\n",
    "        \n",
    "        # Plot on new subplot\n",
    "        if (windowed):\n",
    "            axes[i].plot(dataset[win_num][x_axis], dataset[win_num][y_axis])\n",
    "        else:\n",
    "            axes[i].plot(dataset[x_axis], dataset[y_axis])\n",
    "        \n",
    "        axes[i].set_title(label)\n",
    "        axes[i].set_xlabel(x_axis)\n",
    "        axes[i].set_ylabel(y_axis)\n",
    "        \n",
    "        # Only go two columns wide\n",
    "        col += 1\n",
    "        if (col == 2):\n",
    "            row += 1\n",
    "            col = 0\n",
    "        \n",
    "    plt.subplots_adjust(hspace=0.35, wspace=0.15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot all Z Accel Frame data vs Epoch Time\n",
    "datasets_to_plot = {label: dataset for label, dataset in raw_datasets.items() if 'Frame' in label}\n",
    "plot_all(datasets_to_plot, x_axis='Epoch Time', y_axis='Z Accel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) - Removing Gravity Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove gravitational acceleration from Frame and Middle Phone data\n",
    "## Can't remove from wheel-mounted devices because they rotate over time\n",
    "for label, dataset in raw_datasets.items():\n",
    "    if ('Middle' in label):\n",
    "        # Calculate gravity from last 2000 datapoints and subtract from Z Zccel\n",
    "        ## We need to calculate it manually because device may not be exactly aligned with gravity\n",
    "        gravity = dataset.loc[-2000:]['Z Accel'].mean()\n",
    "        print(label + ' Gravity = {}'.format(gravity))\n",
    "        \n",
    "        raw_datasets[label]['Z Accel'] = raw_datasets[label]['Z Accel'].apply(lambda x: x - gravity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d) - Converting Between Pandas and Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Convert array of Pandas DataFrames to array of 2D Numpy array'''\n",
    "def pd_to_np(pd_datasets, windowed=False):\n",
    "    np_datasets = {}\n",
    "    \n",
    "    for label, dataset in pd_datasets.items():\n",
    "        np_dataset = []\n",
    "        \n",
    "        # If windowed, convert individual windows to Pandas\n",
    "        if (windowed):\n",
    "            for window in dataset:\n",
    "                np_dataset.append(window.as_matrix()) \n",
    "        else:\n",
    "            np_dataset = dataset.as_matrix()\n",
    "        \n",
    "        np_datasets.update({label: np_dataset})\n",
    "        \n",
    "    return np_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Convert array of 2D Numpy arrays to Pandas Data Frames'''\n",
    "def np_to_pd(np_datasets, windowed=False):\n",
    "    pd_datasets = {}\n",
    "    \n",
    "    for label, dataset in np_datasets.items():\n",
    "        pd_dataset = []\n",
    "        \n",
    "        # Use correct column names\n",
    "        new_columns = get_columns(label)\n",
    "            \n",
    "        # If windowed, convert individual windows to Pandas\n",
    "        if (windowed):\n",
    "            for window in dataset:\n",
    "                pd_dataset.append(pd.DataFrame(data=window, columns=new_columns))\n",
    "                \n",
    "        else:\n",
    "            pd_dataset = pd.DataFrame(data=dataset, columns=new_columns)\n",
    "            \n",
    "        pd_datasets.update({label: pd_dataset})\n",
    "    \n",
    "    return pd_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to NumPy\n",
    "raw_datasets = pd_to_np(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if its constructed correctly\n",
    "print('Number of datasets: {}'.format(len(raw_datasets)))\n",
    "print('Shape of first dataset: {}'.format(raw_datasets[dataset_labels[0]].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of data:\n",
    "\n",
    "`Terrain Dataset Dictionary -> NP Array with Row = Datapoint, Col = Direction | Time`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Filtering\n",
    "\n",
    "Datasheets of smartphone level IMUs suggest that IMU filters data before sending it to the phone, and that the cutoff frequency of this filtering is configurable and changes with update frequency.\n",
    "\n",
    "Based on the IMU update rate of about 20 ms, this cutoff frequency is already close to 40 Hz, so filtering is probably unnecessary. Its hard to be sure about actual values because this is configured by the phone manufacturer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Get frequencies for given label'''\n",
    "def get_frequencies(label):\n",
    "    # Sampling frequency varies for different devices\n",
    "    if ('Frame' in label):\n",
    "        f_samp = 100 # Sampling frequency\n",
    "        f_low = 20 # Low pass cutoff frequency\n",
    "        f_high = 1 # High pass cutoff frequency\n",
    "    else:\n",
    "        f_samp = 200 # Sampling frequency\n",
    "        f_low = 40 # Low pass cutoff frequency\n",
    "        f_high = 1 # High pass cutoff frequency\n",
    "        \n",
    "    return f_samp, f_low, f_high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) - Butterworth Filtering\n",
    "\n",
    "Butterworth filters can be high-pass/low-pass/bandpass, and attempt to have maximally flat frequency response in bandpass.\n",
    "\n",
    "Changing the *cutoff frequency* of the filter affects the smoothness of the graph and amount of ringing. \n",
    "\n",
    "Changing the *order* of the filter can have significant effects on smoothness depending on the cutoff frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a copy of the dictionary first\n",
    "datasets = {}\n",
    "\n",
    "for label, dataset in raw_datasets.items():\n",
    "    datasets.update({label: np.copy(dataset)})\n",
    "\n",
    "# Filter each row of each column\n",
    "for label, dataset in datasets.items():\n",
    "    \n",
    "    f_samp, f_low, f_high = get_frequencies(label)\n",
    "    \n",
    "    w_low = f_low / (f_samp / 2) # Normalized frequency\n",
    "    w_high = f_high / (f_samp / 2)\n",
    "\n",
    "    # Get Butterworth filter parameters\n",
    "    b_butter, a_butter = signal.butter(N=4, Wn=w_low, btype='low')\n",
    "    \n",
    "    for i in range(N_DATA_COL):\n",
    "        dataset[:, i] = signal.filtfilt(b_butter, a_butter, dataset[:, i])\n",
    "\n",
    "# Compare filtered and unfiltered\n",
    "plt.clf()\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(datasets['FrameMiddleCarpet'][:, 2], label='Filtered')\n",
    "plt.plot(raw_datasets['FrameMiddleCarpet'][:, 2], label='Unfiltered')\n",
    "plt.xlim(0, 200)\n",
    "plt.legend()\n",
    "plt.xlabel('Datapoint')\n",
    "plt.ylabel('Z Accel ($m/s^2$)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Check construction\n",
    "print('Num filtered datasets: {}'.format(len(datasets)))\n",
    "print('Shape of first filtered dataset: {}'.format(datasets[dataset_labels[0]].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verify we can convert back to Pandas\n",
    "np_to_pd(datasets, windowed=False)[dataset_labels[0]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot all filtered datasets\n",
    "datasets_to_plot = {label: dataset for label, dataset in datasets.items() if ('Frame' in label)}\n",
    "plot_all(np_to_pd(datasets_to_plot), x_axis='Epoch Time', y_axis='Z Gyro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare filtered and unfiltered data\n",
    "dataset_compare(np_to_pd(raw_datasets), 'FrameMiddleConcrete', \n",
    "                np_to_pd(datasets), 'FrameMiddleConcrete', 'Z Gyro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Time Windows\n",
    "\n",
    "### Part (a) - Finding an optimal time window\n",
    "\n",
    "Strategy is to start with a large time window and work down. \n",
    "\n",
    "Plot classification accuracy and time vs window size and find an optimal size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) - Creating Time Windowed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Create an array of different window sizes, taking out the desired array so don't have to rename things\n",
    "\n",
    "WINDOW_SIZE = 200 # Divide by F_SAMP to get window size in seconds\n",
    "datasets_windowed = {}\n",
    "\n",
    "# Trim excess datapoints, then split into windows\n",
    "for label, dataset in raw_datasets.items():   \n",
    "    n_windows = int(len(dataset) / WINDOW_SIZE)\n",
    "    n_points = n_windows*WINDOW_SIZE\n",
    "    \n",
    "    dataset_windowed = np.resize(dataset, (n_points, dataset.shape[1]))\n",
    "    dataset_windowed = np.split(dataset_windowed, n_windows, axis=0)\n",
    "    \n",
    "    datasets_windowed.update({label: dataset_windowed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if its constructed correctly\n",
    "print('Num windowed datasets: {}'.format(len(datasets_windowed)))\n",
    "print('Num of windows in first dataset: {}'.format(len(datasets_windowed[dataset_labels[0]])))\n",
    "print('Shape of individual window: {}'.format(datasets_windowed[dataset_labels[0]][0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of windowed data:\n",
    "\n",
    "`Terrain Dataset Dictionary -> Window List -> NP Array with Row = Datapoint, Col = Direction | Time`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Try out pandas conversion again\n",
    "np_to_pd(datasets_windowed, windowed=True)['FrameMiddleConcrete'][0].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the first time window\n",
    "datasets_to_plot = {label: dataset for label, dataset in datasets_windowed.items() if ('Frame' in label)}\n",
    "plot_all(np_to_pd(datasets_to_plot, windowed=True), \n",
    "         x_axis='Epoch Time', y_axis='Z Accel', windowed=True, win_num=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - FFT and PSD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Plot tranform of given direction and window'''\n",
    "def plot_set_transforms(datasets_transformed, dirn, win_num, transform_name):\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot all the FFTs in one figure overlaid\n",
    "    for label, dataset in datasets_transformed.items():\n",
    "        plt.plot(dataset[win_num][:, -1], dataset[win_num][:, dirn], \n",
    "                 label=label)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlabel('Frequency ($Hz$)')\n",
    "    if (transform_name == 'FFT'):\n",
    "        plt.ylabel('Amplitude (Normalized to Window Size)')\n",
    "    elif (transform_name == 'PSD'):\n",
    "        plt.ylabel('Amplitude (Log-Scaled)')\n",
    "\n",
    "    plt.title(transform_name +' of ' + get_columns(label)[dirn] + \n",
    "              ', Window {}, Window Size = {} Data Points'.format(win_num, WINDOW_SIZE))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) - FFT\n",
    "Its possible the FFT is not valid due to the non determinisitic update rate. Probably needs some interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets_fft = {}\n",
    "\n",
    "# Find the FFT of each column of each data window of each dataset\n",
    "for label, dataset in datasets_windowed.items():\n",
    "    dataset_fft = []\n",
    "    \n",
    "    for window in dataset:\n",
    "        # Number of frequency bins is half of window size to trim the symmetric higher frequencies\n",
    "        n_bins = int(WINDOW_SIZE / 2)\n",
    "        window_fft = np.zeros((n_bins, N_DATA_COL))\n",
    "        \n",
    "        # Sampling frequency dependent on device\n",
    "        f_samp, f_low, f_high = get_frequencies(label)\n",
    "        \n",
    "        for i in range(N_DATA_COL):\n",
    "            # FFT is normalized to window size, to ensure consistency between datasets\n",
    "            window_fft[:, i] = np.resize(np.abs(np.divide(np.fft.fft(window[:, i]), WINDOW_SIZE)), n_bins)\n",
    "            \n",
    "        freq_col = np.transpose([np.linspace(0.0, f_samp / 2, n_bins)])\n",
    "        \n",
    "        # Append the frequency column\n",
    "        window_fft = np.append(window_fft, freq_col, axis=1)\n",
    "        dataset_fft.append(window_fft)\n",
    "        \n",
    "    datasets_fft.update({label + 'FFT': dataset_fft})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check again if its constructed correctly\n",
    "print('Num of FFT\\'d windowed datasets: {}'.format(len(datasets_fft)))\n",
    "print('Num of FFT\\'d windows in first dataset: {}'.format(len(datasets_fft[dataset_labels[0]+'FFT'])))\n",
    "print('Shape of FFT\\'d individual window: {}'.format(datasets_fft[dataset_labels[0]+'FFT'][0].shape))\n",
    "\n",
    "# Pandas conversion\n",
    "np_to_pd(datasets_fft, windowed=True)['FrameMiddleConcreteFFT'][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot Z Accel of 0th window, Frame only\n",
    "ffts_to_plot = {label: dataset for label, dataset in datasets_fft.items() if 'Frame' in label}\n",
    "plot_set_transforms(ffts_to_plot, win_num=0, dirn=2, transform_name='FFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Butterworth cutoff frequency is pretty noticeable. Theres not an easily recognizable cutoff from the unfiltered data.\n",
    "\n",
    "**The number of frequency bins is dependent on window size, and it seems to affect amplitude too.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot all the Frame FFTs for 0th window\n",
    "plot_all(np_to_pd(ffts_to_plot, windowed=True), x_axis='Frequency', y_axis='Z Accel', windowed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) - PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets_psd = {}\n",
    "datasets_psd_log = {}\n",
    "\n",
    "# Find the PSD and log(PSD) of each column of each data window of each dataset\n",
    "for label, dataset in datasets_windowed.items():\n",
    "    dataset_psd = []\n",
    "    dataset_psd_log = []\n",
    "    \n",
    "    for window in dataset:\n",
    "        # Number of frequency bins is half of window size to trim the symmetric higher frequencies\n",
    "        n_bins = int(WINDOW_SIZE / 2)\n",
    "        window_psd = np.zeros((n_bins, N_DATA_COL))\n",
    "        window_psd_log = np.zeros((n_bins, N_DATA_COL))\n",
    "        \n",
    "        f_samp, f_low, f_high = get_frequencies(label)\n",
    "        \n",
    "        for i in range(N_DATA_COL):\n",
    "            # Normalized PSD - Returns frequencies and power density\n",
    "            freq, Pxx = signal.periodogram(window[:, i], f_samp)\n",
    "            window_psd[:, i] = np.resize(Pxx[1:], n_bins)\n",
    "            window_psd_log[:, i] = np.log10(window_psd[:, i])\n",
    "            \n",
    "        # Append freq column\n",
    "        freq_col = np.transpose([freq[:-1]])\n",
    "        window_psd = np.append(window_psd, freq_col, axis=1)\n",
    "        window_psd_log = np.append(window_psd_log, freq_col, axis=1)\n",
    "        \n",
    "        dataset_psd.append(window_psd)\n",
    "        dataset_psd_log.append(window_psd_log)\n",
    "        \n",
    "    datasets_psd.update({label + 'PSD': dataset_psd})\n",
    "    datasets_psd_log.update({label +'PSDLog': dataset_psd_log})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check again if its constructed correctly\n",
    "print('Num PSD\\'d windowed datasets: {}'.format(len(datasets_psd)))\n",
    "print('Num of PSD\\'d windows in first dataset: {}'.format(len(datasets_psd[dataset_labels[0]+'PSD'])))\n",
    "print('Shape of PSD\\'d individual window: {}'.format(datasets_psd['FrameMiddleConcretePSD'][0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Z Accel of 0th window\n",
    "psds_to_plot = {label: dataset for label, dataset in datasets_psd_log.items() if 'FrameMiddle' in label}\n",
    "plot_set_transforms(psds_to_plot, win_num=0, dirn=5, transform_name='PSD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot all PSDs of Z Accel for 0th window\n",
    "plot_all(np_to_pd(psds_to_plot, windowed=True), x_axis='Frequency', y_axis='X Gyro', windowed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that at this point, data is stored like:\n",
    "\n",
    "`Labelled Terrain Dataset -> Time Window -> 2D NumPy Array Col = Dirn | Frequency)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) - 3D Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Plot the set transforms in 3d'''\n",
    "# TODO: Convert to bar graphs for better representation\n",
    "def plot_transforms_3d(datasets_transformed, win_num, dirn, transform_name):\n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    \n",
    "    # Specific to FFT or PSD\n",
    "    if (transform_name == 'FFT'):\n",
    "        ax.set_zlim(0, 0.8)\n",
    "        ax.set_zlabel('Amplitude (Normalized)')\n",
    "    elif (transform_name == 'PSD'):\n",
    "        ax.set_zlim(-20, 10)\n",
    "        ax.set_zlabel('Amplitude (Log-Scaled)')\n",
    "    \n",
    "    # Change y axis to correspond with whats being plotted\n",
    "    n_datasets = len(datasets_transformed)\n",
    "    ax.set_ylim(0, n_datasets)\n",
    "    subset = np.arange(0, n_datasets, dtype=int)\n",
    "    ax.set_yticks(np.add(subset, 1))\n",
    "    \n",
    "    ax.set_xlabel('Frequency ($Hz$)')\n",
    "    ax.set_title(transform_name + ' of ' + data_columns[dirn] + \n",
    "                 ', Window {}, Window Size = {} Data Points'.format(win_num, WINDOW_SIZE))\n",
    "        \n",
    "    # Plot each dataset FFT\n",
    "    for i, (label, dataset) in enumerate(datasets_transformed.items()):\n",
    "        ax.plot(xs=dataset[win_num][:, -1], ys=dataset[win_num][:, dirn],\n",
    "                zs=i+1, zdir='y', label=label)\n",
    "    \n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot 3d Z Accel FFT of 0th window\n",
    "ffts_to_plot = {label: dataset for label, dataset in datasets_fft.items() if 'Frame' in label}\n",
    "plot_transforms_3d(ffts_to_plot, win_num=0, dirn=2, transform_name='FFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot 3d Z Accel PSD of 0th window\n",
    "psds_to_plot = {label: dataset for label, dataset in datasets_psd_log.items() if 'Frame' in label}\n",
    "plot_transforms_3d(psds_to_plot, win_num=0, dirn=0, transform_name='PSD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d) - Spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Plot a spectogram of data'''\n",
    "def plot_spectogram(set_label, dirn, size=WINDOW_SIZE):\n",
    "    # Get sampling frequency for label\n",
    "    f_samp, f_low, f_high = get_frequencies(set_label)\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    \n",
    "    # Compute spectogram directly using time series data\n",
    "    plt.specgram(datasets[set_label][:, dirn], NFFT=size, Fs=f_samp)\n",
    "    plt.title('Spectrogram of {} for {}'.format(data_columns[dirn], set_label))\n",
    "    plt.xlabel('Data Window')\n",
    "    plt.xticks(label=np.arange(0, len(datasets_windowed[set_label])))\n",
    "    plt.ylabel('Frequency ($Hz$)')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X Accel\n",
    "plot_spectogram('FrameMiddleConcrete', dirn=2, size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Feature Extraction\n",
    "\n",
    "Structure of feature data:\n",
    "\n",
    "`Terrain Dataset Dictionary -> Direction Dictionary -> Pandas Array with Row = Window #, Col = Feature`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feature extraction functions\n",
    "\n",
    "'''L2 norm of an array'''\n",
    "def l2norm(array):\n",
    "    return np.linalg.norm(array, ord=2)\n",
    "\n",
    "'''Correlation of an array with itself'''\n",
    "def autocorr(array):\n",
    "    return np.correlate(array, array)\n",
    "\n",
    "'''Root mean squared of an array'''\n",
    "def rms(array):\n",
    "    return np.sqrt(np.mean(array ** 2))\n",
    "\n",
    "'''Zero crossing rate of an array as a fraction of total size of array'''\n",
    "def zcr(array):\n",
    "    # Locations where array > 0, put -1 and 1 for rising/falling,\n",
    "    # divide by total datapoints\n",
    "    return len(np.nonzero(np.diff(array > 0))[0]) / len(array)\n",
    "\n",
    "'''Mean square frequency'''\n",
    "def msf(freqs, psd_amps):\n",
    "    num = np.sum(np.multiply(np.resize(freq, len(psd_amps)), np.power(psd_amps, 2)))\n",
    "    denom = np.sum(psd_amps)\n",
    "    return np.divide(num, denom)\n",
    "\n",
    "'''Root mean square frequency'''\n",
    "def rmsf(freqs, psd_amps):\n",
    "    return np.sqrt(msf(freqs, psd_amps))\n",
    "\n",
    "'''Frequency center'''\n",
    "def fc(freqs, psd_amps):\n",
    "    num = np.sum(np.multiply(np.resize(freq, len(psd_amps)), psd_amps))\n",
    "    denom = np.sum(psd_amps)\n",
    "    return np.divide(num, denom)\n",
    "\n",
    "'''Variance frequency'''\n",
    "def vf(freqs, psd_amps):\n",
    "    return msf(freqs, psd_amps) - fc(freqs, psd_amps) ** 2\n",
    "\n",
    "'''Root variance frequency'''\n",
    "def rvf(freqs, psd_amps):\n",
    "    return np.sqrt(msf(freqs, psd_amps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Extract given features from everything in dataset'''\n",
    "def feature_all(features, datasets, regular=True):\n",
    "    datasets_feat = {}\n",
    "    \n",
    "    # Calculate features for each window of each column of each dataset\n",
    "    for label, dataset in datasets.items():\n",
    "        directions = {}\n",
    "        \n",
    "        # Loop over data columns\n",
    "        for i, direction in enumerate(get_columns(label)[:N_DATA_COL]):\n",
    "            feats = {}\n",
    "            \n",
    "            if (regular):\n",
    "                '''Execute a function over all windows'''\n",
    "                def function_all_windows(function):\n",
    "                    feat_in_window = []\n",
    "                    \n",
    "                    for window in dataset:\n",
    "                        feat_in_window.append(function(window[:, i]))\n",
    "                    \n",
    "                    return feat_in_window\n",
    "                    \n",
    "            else:\n",
    "                '''Alternate defintion for frequency functions'''\n",
    "                def function_all_windows(function):\n",
    "                    feat_in_window = []\n",
    "                    \n",
    "                    for window in dataset:\n",
    "                        feat_in_window.append(function(window[:, -1], window[:, i]))\n",
    "                    \n",
    "                    return feat_in_window\n",
    "                    \n",
    "                    \n",
    "            # Execute every function over all windows    \n",
    "            for feat_name, feat_func in features.items():\n",
    "                feats.update({feat_name: function_all_windows(feat_func)})\n",
    "            \n",
    "            directions.update({direction: pd.DataFrame.from_dict(feats)})\n",
    "\n",
    "        datasets_feat.update({label.replace('PSD', ''): directions})\n",
    "    \n",
    "    return datasets_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Plot a feature on all terrains for each time window'''\n",
    "def plot_set_features(datasets_feat, dirn, feat_name):\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    for label, dataset in datasets_feat.items():\n",
    "        plt.plot(dataset[dirn][feat_name], label=label)\n",
    "        \n",
    "    plt.ylabel(feat_name)\n",
    "    plt.xlabel('Window #')\n",
    "    plt.title(dirn + ', Window Size = {} Data Points'.format(WINDOW_SIZE))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) - Time Domain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Time domain feature function names and actual names\n",
    "time_features = {'Mean': np.mean, 'Std Dev': np.std,  'L2 Norm': l2norm, 'Autocorrelation': autocorr, \n",
    "                  'Max': np.amax, 'Min' : np.amin, 'Root Mean Squared': rms, 'Zero Crossing Rate': zcr, \n",
    "                  'Skew': stats.skew, 'Excess Kurtosis': stats.kurtosis} \n",
    "N_TIME_FEATS = len(time_features)\n",
    "\n",
    "# Create array of features of each window for each dataset and direction\n",
    "datasets_feat_time = feature_all(time_features, datasets_windowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if its constructed correctly and print some info\n",
    "print('Num datasets: {}'.format(len(datasets_feat_time)))\n",
    "print('Num directions: {}'.format(len(datasets_feat_time[dataset_labels[0]])))\n",
    "print('Shape of first dataset first direction: {}'.format(datasets_feat_time[dataset_labels[0]]['X Gyro'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Max of Z Accel\n",
    "feat_datasets_to_plot = {label: dataset for label, dataset in datasets_feat_time.items() if 'Frame' in label}\n",
    "plot_set_features(feat_datasets_to_plot, dirn='Z Gyro', feat_name='Min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) - Frequency Domain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Time domain feature function names and actual names\n",
    "freq_features = {'Mean Square Frequency': msf, 'Root Mean Square Frequency': rmsf, 'Frequency Center': fc, \n",
    "                 'Variance Frequency': vf, 'Root Variance Frequency': rvf}\n",
    "N_FREQ_FEATS = len(freq_features)\n",
    "\n",
    "# Calculate features for each window of each column of each dataset\n",
    "# Create array of features of each window for each dataset and direction\n",
    "# TODO: Check if psd is correct\n",
    "datasets_feat_freq = feature_all(freq_features, datasets_psd, regular=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if its constructed correctly and print some info\n",
    "print('Num datasets: {}'.format(len(datasets_feat_freq)))\n",
    "print('Num directions: {}'.format(len(datasets_feat_freq[dataset_labels[0]])))\n",
    "print('Shape of one direction: {}'.format(datasets_feat_freq[dataset_labels[0]]['X Gyro'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot RVF of Z Accel\n",
    "feat_datasets_to_plot = {label: feature for label, feature in datasets_feat_freq.items() if 'Frame' in label}\n",
    "plot_set_features(feat_datasets_to_plot, dirn='X Gyro', feat_name='Frequency Center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of very similar shapes i.e. highly correlated variables in both time and frequency. Need to implement feature selection.\n",
    "\n",
    "**Ideally we can implement feature selection over all directions, with FFT + PSD + Time Features + Freq Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert transformed data to Pandas\n",
    "datasets_fft = np_to_pd(datasets_fft, windowed=True)\n",
    "datasets_psd_log = np_to_pd(datasets_psd_log, windowed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Move frequency column to index'''\n",
    "def set_freq_indices(datasets):\n",
    "    for dataset in datasets.values():\n",
    "        for window in dataset:\n",
    "            window = window.set_index('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply frequency indexing to every dataset\n",
    "set_freq_indices(datasets_fft)\n",
    "set_freq_indices(datasets_psd_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Confirm formatting\n",
    "datasets_fft['FrameMiddleConcreteFFT'][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add time and freq features together\n",
    "datasets_feat = {}\n",
    "\n",
    "for label, dataset in datasets_feat_time.items():\n",
    "    dataset_feat = {}\n",
    "    \n",
    "    for dirn_label, dirn_df in dataset.items():\n",
    "        new_df = dirn_df.join(datasets_feat_freq[label][dirn_label], how='outer')\n",
    "        dataset_feat.update({dirn_label: new_df})\n",
    "    \n",
    "    datasets_feat.update({label: dataset_feat})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Confirm formatting\n",
    "datasets_feat['FrameMiddleAsphalt']['X Gyro'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) - Standardization\n",
    "\n",
    "Standardize each feature to mean 0 and standard deviation 1. This will make PCA and classification easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Normalize an already featured dataset'''\n",
    "def normalize_datasets(datasets, windowed=False):\n",
    "    for label, dataset in datasets.items():\n",
    "        if windowed:\n",
    "            for window_df in dataset:\n",
    "                window_df = window_df.dropna()\n",
    "                window_df = window_df.apply(pre.scale)\n",
    "        else:        \n",
    "            for dirn_label, dirn_df in dataset.items():\n",
    "                dirn_df = dirn_df.dropna()\n",
    "                dirn_df = dirn_df.apply(pre.scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# So we can remove infinites\n",
    "pd.set_option('use_inf_as_null', True)\n",
    "\n",
    "normalize_datasets(datasets_feat)\n",
    "normalize_datasets(datasets_fft, windowed=True)\n",
    "normalize_datasets(datasets_psd_log, windowed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot normalized features\n",
    "feat_datasets_to_plot = {label: dataset for label, dataset in datasets_feat.items() if 'Frame' in label}\n",
    "plot_set_features(feat_datasets_to_plot, dirn='Z Accel', feat_name='Mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot normalized fft\n",
    "ffts_to_plot = {label: dataset for label, dataset in datasets_fft.items() if 'Frame' in label}\n",
    "plot_set_transforms(pd_to_np(ffts_to_plot, windowed=True), dirn=2, win_num=0, transform_name='FFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) - Adding Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'Add labels to a dataset'\n",
    "def insert_labels(datasets, windowed=False):\n",
    "    # Add to each dataframe of a dataset\n",
    "    for i, (label, dataset) in enumerate(datasets.items()):\n",
    "        # Either formatted in windows (transforms) or dict->dict (features)\n",
    "        if (windowed):\n",
    "            for window_df in dataset:\n",
    "                labels = [get_terrain(label) for _ in range(len(window_df))]\n",
    "                window_df.insert(0, 'Label', labels)\n",
    "        \n",
    "        else:\n",
    "            for dirn_label, dirn_df  in dataset.items():\n",
    "                labels = [get_terrain(label) for _ in range(len(dirn_df))]\n",
    "                dirn_df.insert(0, 'Label', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add labels\n",
    "insert_labels(datasets_feat)\n",
    "insert_labels(datasets_fft, windowed=True)\n",
    "insert_labels(datasets_psd_log, windowed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check labelled transform data\n",
    "datasets_fft['FrameMiddleAsphaltFFT'][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check labelled feature data\n",
    "datasets_feat['FrameMiddleAsphalt']['X Gyro'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) - mRMR\n",
    "\n",
    "Necessary because of the number of different features/transforms in each direction.\n",
    "\n",
    "Try to find which features are most relevant, from all directions.\n",
    "\n",
    "**Start with just featured datasets from frames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_dirns(datasets):\n",
    "    datasets_combined = {}\n",
    "    \n",
    "    for label, dataset in datasets.items():\n",
    "        df_combined = pd.DataFrame()\n",
    "        \n",
    "        for dirn_label, dirn_df in dataset.items():\n",
    "            df_combined = df_combined.join(dirn_df, how='outer', rsuffix=' '+dirn_label)\n",
    "        \n",
    "        datasets_combined.update({label: df_combined})\n",
    "    \n",
    "    return datasets_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take frame data and combine directions\n",
    "datasets_frame_feat = {label: dataset for label, dataset in datasets_feat.items() if 'Frame' in label}\n",
    "datasets_frame_feat = combine_dirns(datasets_frame_feat)\n",
    "\n",
    "datasets_frame_feat['FrameMiddleConcrete'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_datasets(datasets):\n",
    "    return pd.concat(list(datasets.values()), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combine_datasets(datasets_frame_feat).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glossary\n",
    "\n",
    "`Dataset` - Batch of data recorded on one terrain type\n",
    "\n",
    "`Data Window` - Split up portion of a `Dataset`\n",
    "\n",
    "`Direction` - Linear acceleration or gyroscope in $x,y$ or $z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
