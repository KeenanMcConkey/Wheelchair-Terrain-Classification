{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terrain Classification - General Data\n",
    "### Created by Keenan McConkey 2019.8.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "\n",
    "from datetime import datetime\n",
    "from decimal import Decimal\n",
    "\n",
    "import pymrmr\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Importing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (0) - Functions for Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easier to read column names\n",
    "std_columns = ['X Accel', 'Y Accel', 'Z Accel', 'X Gyro', 'Y Gyro', 'Z Gyro', 'Run Time', 'Epoch Time']\n",
    "data_columns =  ['X Accel', 'Y Accel', 'Z Accel', 'X Gyro', 'Y Gyro', 'Z Gyro']\n",
    "synthesis_columns = ['Calc X Vel', 'Calc Z Gyro', 'Run Time', 'Epoch Time',\n",
    "                     'Left X Accel', 'Left Y Accel', 'Left Z Accel', 'Left XY Accel', \n",
    "                     'Left X Gyro','Left Y Gyro', 'Left Z Gyro',\n",
    "                     'Right X Accel', 'Right Y Accel', 'Right Z Accel', 'Right XY Accel',\n",
    "                     'Right X Gyro', 'Right Y Gyro', 'Right Z Gyro']\n",
    "\n",
    "# Columns not currently used for classification\n",
    "unused_columns = ['Time Received', 'Timestamp', 'Pitch (Deg)', 'Roll (Deg)', 'Heading (Deg)',\n",
    "                  'MagX', 'MagY', 'MagZ', 'ACCELEROMETER XY (m/s²)']\n",
    "# Differs for synthesis data (as usual)\n",
    "unused_columns_synth = ['ACCELEROMETER X (m/s²)']\n",
    "\n",
    "'''Get columns for given label'''\n",
    "def get_columns(label):\n",
    "    # Columns differ for synthesis data\n",
    "    if 'Middle' in label or 'Left' in label or 'Right' in label:\n",
    "        columns = std_columns.copy()\n",
    "    elif 'Synthesis' in label:\n",
    "        columns = synthesis_columns.copy()\n",
    "    else:\n",
    "        raise Exception('Unknown label')\n",
    "    \n",
    "    # For transformed datasets replace time columns with frequency\n",
    "    if 'FFT' in label or 'PSD' in label:\n",
    "        columns.remove('Epoch Time')\n",
    "        \n",
    "        if 'Run Time' in columns:\n",
    "            columns.remove('Run Time')\n",
    "        \n",
    "        columns.append('Frequency')\n",
    "        \n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of terrains, placements, and transforms used\n",
    "terrains = ['Concrete', 'Carpet', 'Linoleum', 'Asphalt', 'Sidewalk', 'Grass', 'Gravel']\n",
    "placements = ['Left', 'Right', 'Middle', 'Synthesis']\n",
    "transforms = ['FFT', 'PSDLog', 'PSD']\n",
    "movements = ['F8', 'Donut', 'Straight']\n",
    "\n",
    "'''Get the integer terrain value of a given label'''\n",
    "def get_terrain_num(_label):\n",
    "    for i, terrain in enumerate(terrains):\n",
    "        if terrain in _label:\n",
    "            return i\n",
    "        \n",
    "    raise Exception('Unknown terrain')\n",
    "\n",
    "'''Get the name associated with a terrain integer'''\n",
    "def get_terrain_name(terrain_num):\n",
    "    return terrains[terrain_num]\n",
    "\n",
    "'''Get the placement location name for given label'''\n",
    "def get_placement(_label):\n",
    "    for placement in placements:\n",
    "        if placement in _label:\n",
    "            return placement\n",
    "    \n",
    "    raise Exception('Unknown placement')\n",
    "\n",
    "'''Get the transform used for given label'''\n",
    "def get_transform(_label):\n",
    "    for transform in transforms:\n",
    "        if transform in _label:\n",
    "            return transform\n",
    "    \n",
    "    raise Exception('Unkown transform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) - Parsing Data into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find .csv files\n",
    "# TODO: Split according to user and create a dictionary\n",
    "raw_datasets = {}\n",
    "glob_paths = glob.glob('imu_data/new_setup/set_manual/*.csv')\n",
    "\n",
    "# Remove 9250 IMU data (for now)\n",
    "dataset_paths = [path for path in glob_paths if '9250' not in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import datasets as a dictionary of Pandas DataFrames\n",
    "for dataset_path in dataset_paths:\n",
    "    \n",
    "    # Parse labels from filenames\n",
    "    dataset_label = dataset_path.split('/')[-1].split('.')[0]    \n",
    "\n",
    "    # Read from CSV to Pandas\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Drop unused columns\n",
    "    if 'Synthesis' in dataset_label:\n",
    "        unused = [unused_column for unused_column in unused_columns_synth if unused_column in dataset.columns]\n",
    "    else:\n",
    "        unused = [unused_column for unused_column in unused_columns if unused_column in dataset.columns]\n",
    "    dataset = dataset.drop(unused, axis='columns')\n",
    "    \n",
    "    # Rename columns to easier to work with names\n",
    "    dataset.columns = get_columns(dataset_label)\n",
    "\n",
    "    # Convert timestamps to epoch time in sec\n",
    "    dataset['Epoch Time'] = dataset['Epoch Time'].apply(datetime.strptime, args=(\"%Y-%m-%d %H:%M:%S:%f\", ))\n",
    "    dataset['Epoch Time'] = dataset['Epoch Time'].apply(datetime.timestamp)\n",
    "    \n",
    "    # Remove gravitational acceleration from Middle frame data\n",
    "    ## Can't remove from wheel-mounted Left and Right wheel data because they rotate over time\n",
    "    \n",
    "    # Eventually want to move this to Raspberry Pi using accuracy orientation calculation\n",
    "    if 'Middle' in dataset_label:\n",
    "        # Remove gravity from z component of acceleration, \n",
    "        g_z = 9.81\n",
    "        dataset['Z Accel'] = dataset['Z Accel'].apply(lambda x: x - g_z)\n",
    "\n",
    "    # Remove dropped bytes - i.e. run times below zero\n",
    "    #dataset = dataset.drop(dataset.loc[dataset['Run Time'] < 0].index.values.tolist())\n",
    "\n",
    "    # Reorganize synthesis data columns to put Run Time and Epoch Time at the end\n",
    "    if 'Synthesis' in dataset_label:\n",
    "        dataset_columns = dataset.columns.tolist()\n",
    "        dataset_columns = dataset_columns[:3] + dataset_columns[5:] + dataset_columns[3:5]\n",
    "        dataset = dataset[dataset_columns]\n",
    "    \n",
    "    # Trim edges to account for start and end time\n",
    "    # Use different axes to check threshold for synthesis data\n",
    "    if 'Synthesis' in dataset_label:\n",
    "        thresh_axes = 'Calc X Vel'\n",
    "        STARTUP_THRESH = 0.1\n",
    "    else:\n",
    "        thresh_axes = 'Z Accel'\n",
    "        STARTUP_THRESH = 2.0\n",
    "        \n",
    "    # Caluclate first and last instance above threshold and use as the time domain\n",
    "    start_index = dataset[dataset[thresh_axes] > STARTUP_THRESH].index[0]\n",
    "    stop_index = dataset[dataset[thresh_axes] > STARTUP_THRESH].index[-1]\n",
    "    dataset = dataset[start_index:stop_index]\n",
    "    \n",
    "    # Datasets are stored in a dictionary\n",
    "    raw_datasets.update({dataset_label: dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update order of synthesis columns\n",
    "synthesis_columns = ['Calc X Vel', 'Calc Z Gyro',\n",
    "                     'Left X Accel', 'Left Y Accel', 'Left Z Accel', 'Left XY Accel', \n",
    "                     'Left X Gyro','Left Y Gyro', 'Left Z Gyro',\n",
    "                     'Right X Accel', 'Right Y Accel', 'Right Z Accel', 'Right XY Accel',\n",
    "                     'Right X Gyro', 'Right Y Gyro', 'Right Z Gyro',\n",
    "                     'Run Time', 'Epoch Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raw datasets: 0\n"
     ]
    }
   ],
   "source": [
    "# Sort dictionary according to keys\n",
    "raw_datasets = {label: raw_datasets[label] for label in sorted(raw_datasets.keys())}\n",
    "\n",
    "# Save list of keys to variable\n",
    "dataset_labels = list(raw_datasets.keys())\n",
    "print('Number of raw datasets: {}'.format(len(dataset_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c10237c936d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Check dataset formatting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mraw_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Check dataset formatting\n",
    "raw_datasets[dataset_labels[0]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) - Visualizing Time Domain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plot a single Pandas dataset for given x and y axes'''\n",
    "def plot_one(_datasets, dataset_name, x_axis, y_axis, xlim=None, ylim=None, save_fig=False):\n",
    "    # Figure parameters\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(dataset_name)\n",
    "    plt.xlabel(x_axis + ' (s)')\n",
    "    \n",
    "    # Add relevant units to y label\n",
    "    if 'Accel' in y_axis:\n",
    "        plt.ylabel(y_axis + ' ($m/s^2$)')\n",
    "    elif 'Gyro' in y_axis:\n",
    "        plt.ylabel(y_axis + ' ($rad/s$)')\n",
    "    elif 'Vel' in y_axis:\n",
    "        plt.ylabel(y_axis + ' ($m/s$)')\n",
    "    else:\n",
    "        plt.ylabel('Unknown')\n",
    "    \n",
    "    # Use limits if they've been passed in\n",
    "    if xlim is not None:\n",
    "        plt.xlim(xlim[0], xlim[1])\n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim[0], ylim[1])\n",
    "    \n",
    "    # Plot relevant data\n",
    "    plt.plot(_datasets[dataset_name][x_axis], _datasets[dataset_name][y_axis])\n",
    "    \n",
    "    # Save figure to png file without plotting\n",
    "    if save_fig:\n",
    "        save_name = '/home/caris/Wheelchair-Terrain-Classification/imgs/'\n",
    "        save_name = save_name + y_axis.replace(' ', '_') + '_' + dataset_name + '.png'\n",
    "        plt.savefig(save_name)\n",
    "    \n",
    "    # Just plot without saving\n",
    "    else:    \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot some raw data\n",
    "plot_one(raw_datasets, dataset_labels[0], 'Run Time', 'X Accel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot every dataset Z Accel and save images\n",
    "#for label in dataset_labels:\n",
    "#    plot_one(raw_datasets, label, 'Run Time', 'Z Accel', save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Compare two Pandas datasets by Epoch Time'''\n",
    "def dataset_compare(dataset1, label1, dataset2, label2, y_axis, t_offset=0, y_offset=0):\n",
    "    # Plot parameters\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(figsize=(20, 5))\n",
    "    ax.set_xlabel('Epoch Time ($s$)')\n",
    "    ax.set_title(y_axis + ' for ' + label1 + ' and ' + label2)\n",
    "        \n",
    "    # Add relevant units to y label\n",
    "    if 'Gyro' in y_axis:\n",
    "        ax.set_ylabel(y_axis + ' ($rad/s$)')\n",
    "    elif 'Accel' in y_axis:\n",
    "        ax.set_ylabel(y_axis + ' ($m/s^2$)')\n",
    "    elif 'Vel' in y_axis:\n",
    "        ax.set_ylabel(y_axis + ' ($m/s$)')\n",
    "    else:\n",
    "        ax.set_ylabel('Unknown')\n",
    "    \n",
    "    # Plot data with given y and t offsets applied to first dataset\n",
    "    ax.plot(dataset1[label1]['Epoch Time'].apply(lambda t: t + t_offset), \n",
    "            dataset1[label1][y_axis].apply(lambda y: y + y_offset), label=label1)\n",
    "    ax.plot(dataset2[label2]['Epoch Time'], \n",
    "            dataset2[label2][y_axis], label=label2)\n",
    "    \n",
    "    # Include offset info text in plot\n",
    "    offset_text = 'Offsets\\n'\n",
    "    offset_text += ': t={}'.format(t_offset) + ', ' + 'y={}'.format(y_offset)\n",
    "    ax.text(0.05, 0.05, s=offset_text, \n",
    "            horizontalalignment='left', verticalalignment='bottom', transform=ax.transAxes)\n",
    "    \n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compare Z Gyro data for each wheel in the same dataset\n",
    "#dataset_compare(raw_datasets, 'Left_ConcreteStraightKevin_Module',\n",
    "#                raw_datasets, 'Right_ConcreteStraightKevin_Module', \n",
    "#                dirn='Z Gyro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plot given x and y axes for every Pandas DataFrame in given array of datasets'''\n",
    "def plot_all(_datasets, x_axis, y_axis, windowed=False, win_num=0, take_row=False):\n",
    "    plt.clf()\n",
    "    \n",
    "    # Set paramaeters based on number of datasets to plot\n",
    "    n_axes = len(_datasets)\n",
    "    odd_axes = n_axes % 2 == 0\n",
    "    rows = int((n_axes + 1) / 2)\n",
    "    \n",
    "    # Scale approriately\n",
    "    if (odd_axes):\n",
    "        fig = plt.figure(figsize=(n_axes*5, n_axes*3))          \n",
    "    else:\n",
    "        fig = plt.figure(figsize=(n_axes*5, n_axes*2))\n",
    "    \n",
    "    # Grid of subplots\n",
    "    gs = gridspec.GridSpec(rows, 2)\n",
    "    axes = []\n",
    "    row, col = 0, 0\n",
    "    \n",
    "    # Plot each of the given datasets\n",
    "    for i, (label, dataset) in enumerate(_datasets.items()):\n",
    "        # Take a whole row if odd num of axes\n",
    "        if (i == n_axes-1 and odd_axes and take_row): \n",
    "            axes.append(fig.add_subplot(gs[row, :]))\n",
    "        else:\n",
    "            axes.append(fig.add_subplot(gs[row, col]))\n",
    "        \n",
    "        # Plot on new subplot\n",
    "        if (windowed):\n",
    "            axes[i].plot(dataset[win_num][x_axis], dataset[win_num][y_axis])\n",
    "        else:\n",
    "            axes[i].plot(dataset[x_axis], dataset[y_axis])\n",
    "        axes[i].set_title(label)\n",
    "        axes[i].set_xlabel(x_axis + ' (s)')\n",
    "        \n",
    "        if 'Gyro' in y_axis:\n",
    "            axes[i].set_ylabel(y_axis + ' ($rad/s$)')\n",
    "        elif 'Accel' in y_axis:\n",
    "            axes[i].set_ylabel(y_axis + ' ($m/s^2$)')\n",
    "        \n",
    "        # Only go two columns wide\n",
    "        col += 1\n",
    "        if (col == 2):\n",
    "            row += 1\n",
    "            col = 0\n",
    "        \n",
    "    plt.subplots_adjust(hspace=0.35, wspace=0.15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot Z Accel of same dataset for different modules\n",
    "datasets_to_plot = {label: dataset for label, dataset in raw_datasets.items() if 'Synthesis' in label and 'Grass' in label and 'Kevin' in label}\n",
    "plot_all(datasets_to_plot, x_axis='Run Time', y_axis='Calc X Vel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) - Converting Between Pandas and Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check whether its necessary to convert to Numpy\n",
    "\n",
    "'''Convert array of Pandas DataFrames to array of 2D NumPy array'''\n",
    "def pd_to_np(pd_datasets, windowed=False):\n",
    "    np_datasets = {}\n",
    "    \n",
    "    # Convert each dataset individually\n",
    "    for label, dataset in pd_datasets.items():\n",
    "        np_dataset = []\n",
    "        \n",
    "        # Return passed datasets if they are already NumPy ndarrays\n",
    "        if type(dataset) is np.ndarray:\n",
    "            print('Note: Already a NumPy array!')\n",
    "            return pd_datasets\n",
    "        \n",
    "        # If windowed, convert individual windows to Pandas\n",
    "        if (windowed):\n",
    "            for window in dataset:\n",
    "                np_dataset.append(window.as_matrix()) \n",
    "        else:\n",
    "            np_dataset = dataset.to_numpy()\n",
    "        \n",
    "        np_datasets.update({label: np_dataset})\n",
    "        \n",
    "    return np_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Convert array of 2D NumPy arrays to Pandas Data Frames'''\n",
    "def np_to_pd(np_datasets, windowed=False):\n",
    "    pd_datasets = {}\n",
    "    \n",
    "    # Convert each dataset individually\n",
    "    for label, dataset in np_datasets.items():\n",
    "        pd_dataset = []\n",
    "        \n",
    "        # Return passed datasets if they are already Pandas dataframes\n",
    "        if type(dataset) is pd.DataFrame:\n",
    "            print('Note: Already a Pandas dataframe!')\n",
    "            return np_datasets\n",
    "        \n",
    "        # Use correct column names\n",
    "        new_columns = get_columns(label)\n",
    "            \n",
    "        # If windowed, convert individual windows to Pandas\n",
    "        if (windowed):\n",
    "            for window in dataset:\n",
    "                pd_dataset.append(pd.DataFrame(data=window, columns=new_columns))     \n",
    "        else:\n",
    "            pd_dataset = pd.DataFrame(data=dataset, columns=new_columns)\n",
    "            \n",
    "        pd_datasets.update({label: pd_dataset})\n",
    "    \n",
    "    return pd_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to NumPy\n",
    "raw_datasets = pd_to_np(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to convert back to Pandas\n",
    "#raw_datasets = np_to_pd(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if its constructed correctly\n",
    "print('Number of datasets: {}'.format(len(raw_datasets)))\n",
    "print('Shape of first dataset: {}'.format(raw_datasets[dataset_labels[0]].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Structure:\n",
    "\n",
    "`Terrain Dataset Dictionary \n",
    "-> NP Array\n",
    "---> Row = Datapoint, \n",
    "---> Col = Direction | Time`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Filtering\n",
    "\n",
    "#### Right now the difference in sampling rate is a limitation because we can't integrate frame and wheel data. Reccomend finding a common sampling rate for all modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get relevant frequencies for given label based on whether its a frame or wheel dataset'''\n",
    "def get_frequencies(label):\n",
    "    # Sampling frequency (and thus cutoff frequency) varies between frame and wheel\n",
    "    ## Currently not using high pass frequency\n",
    "    if 'Left' in label or 'Right' in label or 'Synthesis' in label:\n",
    "        f_samp = 333.3 # Sampling frequency\n",
    "        f_low = 60 # Low pass cutoff frequency\n",
    "        f_high = 1 # High pass cutoff frequency\n",
    "    elif 'Middle' in label:\n",
    "        f_samp = 300 # Sampling frequency\n",
    "        f_low = 55 # Low pass cutoff frequency\n",
    "        f_high = 1 # High pass cutoff frequency\n",
    "    else:\n",
    "        raise Exception('Unknown label')\n",
    "        \n",
    "    return f_samp, f_low, f_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get window size based on whether label is for a frame or wheel dataset'''\n",
    "def get_window_size(label):\n",
    "    # Sampling frequency differs varies between frame and wheel modules\n",
    "    # Synthesis data is created from combining left and right wheel data so it has the same samp rate\n",
    "    if 'Left' in label or 'Right' in label or 'Synthesis' in label:\n",
    "        window_size = 333\n",
    "    elif 'Middle' in label:\n",
    "        window_size = 300\n",
    "    else:\n",
    "        raise Exception('Unknown label')\n",
    "        \n",
    "    return window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get number of data columns in dataset with given label'''\n",
    "def get_n_data_col(label):\n",
    "    # Different number of columns for Synthesis datasets\n",
    "    if 'Synthesis' in label:\n",
    "        n_col = 16\n",
    "    elif 'Left' in label or 'Right' in label or 'Middle' in label:\n",
    "        n_col = 6\n",
    "    else:\n",
    "        raise Exception('Unknown label')\n",
    "    return n_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) - Butterworth Filtering\n",
    "\n",
    "Butterworth filters can be high-pass/low-pass/bandpass, and attempt to have maximally flat frequency response in bandpass.\n",
    "\n",
    "Changing the *cutoff frequency* of the filter affects the smoothness of the graph and amount of ringing. \n",
    "\n",
    "Changing the *order* of the filter can have significant effects on smoothness depending on the cutoff frequency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nominally set to 30 % of Nyquist Freq**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filtered datasets dictionary\n",
    "datasets = {}\n",
    "\n",
    "# Filter each dataset individually\n",
    "for label, raw_dataset in raw_datasets.items():\n",
    "    # Sampling rates are not consistent across all datasets\n",
    "    f_samp, f_low, f_high = get_frequencies(label)\n",
    "    \n",
    "    # Get normalized frequencies\n",
    "    w_low = f_low / (f_samp / 2) \n",
    "    w_high = f_high / (f_samp / 2)\n",
    "\n",
    "    # Get Butterworth filter parameters\n",
    "    # Note low pass filter right now, could change to bandpass eventually if desired\n",
    "    b_butter, a_butter = signal.butter(N=4, Wn=w_low, btype='low')\n",
    "    \n",
    "    # Number of columns containing data\n",
    "    n_data_col = get_n_data_col(label)\n",
    "    \n",
    "    # Filter all the data columns\n",
    "    dataset = np.copy(raw_dataset)\n",
    "    for i in range(n_data_col):\n",
    "        dataset[:, i] = signal.filtfilt(b_butter, a_butter, dataset[:, i])\n",
    "        \n",
    "    datasets.update({label: dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check construction of filtered dataset\n",
    "print('Num filtered datasets: {}'.format(len(datasets)))\n",
    "print('Shape of first filtered dataset: {}'.format(datasets[dataset_labels[0]].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we can convert back to Pandas\n",
    "np_to_pd(datasets, windowed=False)[dataset_labels[0]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Again compare filtered and unfiltered data\n",
    "dataset_compare(np_to_pd(raw_datasets), dataset_labels[0], \n",
    "                np_to_pd(datasets), dataset_labels[0], 'X Accel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) - Further Time Domain Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Compare Z Gyro of frame to Z Gyro calculated by combining wheel data, for Pandas data'''\n",
    "def gyro_compare(dataset, label, xlim=None):\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Calculate Z Gyro using wheel data\n",
    "    r_wheel = 0.30\n",
    "    d_base = 0.54\n",
    "    calc = (r_wheel / d_base)*(-1 * dataset['Left_' + label]['Z Gyro'] - dataset['Right_' + label]['Z Gyro'])\n",
    "    calc = np.reshape(calc, dataset['Right_' + label]['Epoch Time'].shape)\n",
    "    \n",
    "    # Plot actual frame gyro, calculated frame gyro, and actual wheel gyro\n",
    "    ax.plot(dataset['Middle_' + label]['Epoch Time'], dataset['Middle_' + label]['Z Gyro'], \n",
    "            label='Actual Frame')\n",
    "    ax.plot(dataset['Right_' + label]['Epoch Time'], calc, label='Calculated Frame')\n",
    "    ax.plot(dataset['Left_' + label]['Epoch Time'], dataset['Left_' + label]['Z Gyro'], \n",
    "            label='Actual L Wheel')\n",
    "    ax.plot(dataset['Right_' + label]['Epoch Time'], dataset['Right_' + label]['Z Gyro'],\n",
    "            label='Actual R Wheel')\n",
    "    \n",
    "    # Other figure stuff\n",
    "    ax.set_xlabel('Epoch Time ($s$)')\n",
    "    ax.set_ylabel('Z Gyro ($m/s^2$)')\n",
    "    ax.set_title('Comparing Actual to Calculated Z Gyro Data')\n",
    "    \n",
    "    if xlim:\n",
    "        start_time = dataset['Middle_' + label]['Epoch Time'][0]\n",
    "        ax.set_xlim(xlim[0] + start_time, xlim[1] + start_time)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gyro_compare(np_to_pd(datasets), 'ConcreteDonutKevin_Module', xlim=(20, 40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Time Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) - Creating Time Windowed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_windowed = {}\n",
    "\n",
    "# Trim excess datapoints, then split into windows\n",
    "for label, dataset in datasets.items():\n",
    "    window_size = get_window_size(label)\n",
    "    n_windows = int(len(dataset) / window_size) # Total windows that fit in the dataset\n",
    "    n_points = n_windows * window_size # Total number of points that fit in non-overlapping windows\n",
    "    \n",
    "    # Windowed data\n",
    "    dataset_windowed = []\n",
    "    \n",
    "    # Iterate through dataset by half a window at a time and extract windows\n",
    "    i = 0\n",
    "    window_slide = int(window_size / 2)\n",
    "    \n",
    "    while (i < n_points):\n",
    "        dataset_windowed.append(dataset[i:i + window_size])\n",
    "        i += window_slide\n",
    "    \n",
    "    datasets_windowed.update({label: dataset_windowed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if its constructed correctly\n",
    "print('Num windowed datasets: {}'.format(len(datasets_windowed)))\n",
    "print('Num of windows in first dataset: {}'.format(len(datasets_windowed[dataset_labels[0]])))\n",
    "print('Shape of individual window: {}'.format(datasets_windowed[dataset_labels[0]][0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Try out Pandas conversion again\n",
    "np_to_pd(datasets_windowed, windowed=True)[dataset_labels[0]][0].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first time window\n",
    "datasets_to_plot = {label: dataset for label, dataset in datasets_windowed.items() if 'Synthesis' in label and 'F8' in label and 'Kevin' in label}\n",
    "plot_all(np_to_pd(datasets_to_plot, windowed=True), \n",
    "         x_axis='Run Time', y_axis='Calc X Vel', windowed=True, win_num=102)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Structure\n",
    "\n",
    "`Terrain Dataset Dictionary\n",
    "-> Data Window List\n",
    "---> NP Array\n",
    "-----> Row = Datapoint\n",
    "-----> Col = Direction | Time`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Butterworth filter bandpass isn't a perfect edge so there is still some relevant data past the cutoff\n",
    "N_BINS_OVER_CUTOFF = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plot transform of given direction and window of a windowed NumPy dataset dictionary'''\n",
    "def plot_set_transforms(datasets_transformed, dirn, win_num, transform_name):\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot all the FFTs in one figure overlaid\n",
    "    for label, dataset in datasets_transformed.items():\n",
    "        plt.plot(dataset[win_num][:, -1], dataset[win_num][:, dirn], \n",
    "                 label=label)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlabel('Frequency ($Hz$)')\n",
    "    if (transform_name == 'FFT'):\n",
    "        plt.ylabel('Amplitude (Normalized to Window Size)')\n",
    "    elif (transform_name == 'PSD'):\n",
    "        plt.ylabel('Amplitude (Log-Scaled)')\n",
    "\n",
    "    plt.title(transform_name +' of ' + get_columns(label)[dirn] + ', Window {}'.format(win_num))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) - FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets_fft = {}\n",
    "\n",
    "# Find the FFT of each column of each data window of each dataset\n",
    "for label, dataset in datasets_windowed.items():\n",
    "    dataset_fft = []\n",
    "    \n",
    "    for window in dataset:\n",
    "        # Get relevant parameters for given label\n",
    "        f_samp, f_low, f_high = get_frequencies(label)        \n",
    "        n_data_col = get_n_data_col(label)\n",
    "        \n",
    "        # Only include frequency bins up to and a little bit past the cutoff frequency\n",
    "        # Everything past that is useless because its the same on all terrains\n",
    "        window_size = get_window_size(label)\n",
    "        n_bins = int(window_size / f_samp * f_low) + N_BINS_OVER_CUTOFF\n",
    "        window_fft = np.zeros((n_bins, n_data_col))\n",
    "        \n",
    "        for i in range(n_data_col):\n",
    "            # FFT is normalized to window size, to ensure consistency between different size choices\n",
    "            window_fft[:, i] = np.resize(np.abs(np.divide(np.fft.fft(window[:, i]), window_size)), n_bins)\n",
    "            \n",
    "        # Get positive frequency bins for given FFT parameters\n",
    "        freq_col = np.transpose([np.resize(np.fft.fftfreq(window_size, 1 / f_samp), n_bins)])\n",
    "        \n",
    "        # Append the frequency column\n",
    "        window_fft = np.append(window_fft, freq_col, axis=1)\n",
    "        dataset_fft.append(window_fft)\n",
    "        \n",
    "    datasets_fft.update({label + '_FFT': dataset_fft})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check again if its constructed correctly\n",
    "print('Num of FFT\\'d windowed datasets: {}'.format(len(datasets_fft)))\n",
    "print('Num of FFT\\'d windows in first dataset: {}'.format(len(datasets_fft[dataset_labels[4]+'_FFT'])))\n",
    "print('Shape of FFT\\'d individual window: {}'.format(datasets_fft[dataset_labels[4]+'_FFT'][0].shape))\n",
    "\n",
    "# Test Pandas conversion\n",
    "np_to_pd(datasets_fft, windowed=True)[dataset_labels[0]+'_FFT'][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compare some FFTs\n",
    "ffts_to_plot = {label: dataset for label, dataset in datasets_fft.items() if 'Middle' in label and 'F8' in label and 'Kevin' in label}\n",
    "plot_set_transforms(ffts_to_plot, win_num=0, dirn=2, transform_name='FFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The number of frequency bins is dependent on window size, and it seems to affect amplitude too.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare some FFTs\n",
    "plot_all(np_to_pd(ffts_to_plot, windowed=True), x_axis='Frequency', y_axis='Z Accel', windowed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) - PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_psd = {}\n",
    "datasets_psd_log = {}\n",
    "\n",
    "# Find the PSD and log(PSD) of each column of each data window of each dataset\n",
    "for label, dataset in datasets_windowed.items():\n",
    "    dataset_psd = []\n",
    "    dataset_psd_log = []\n",
    "    \n",
    "    for window in dataset:\n",
    "        # Frequencies depedent on dataset and device used\n",
    "        f_samp, f_low, f_high = get_frequencies(label)\n",
    "        \n",
    "        # Number of columns of data different for synthesis data\n",
    "        n_data_col = get_n_data_col(label)\n",
    "        \n",
    "        # Only include frequency bins up to and a little bit past the cutoff frequency\n",
    "        # Everything past that is useless because its the same on all terrains\n",
    "        window_size = get_window_size(label)\n",
    "        n_bins = int(window_size / f_samp * f_low) + N_BINS_OVER_CUTOFF\n",
    "        window_psd = np.zeros((n_bins, n_data_col))\n",
    "        window_psd_log = np.zeros((n_bins, n_data_col))\n",
    "        \n",
    "        # Calculate PSD for each axes\n",
    "        for i in range(n_data_col):\n",
    "            # Normalized PSD - Returns frequencies and power density\n",
    "            freq, Pxx = signal.periodogram(window[:, i], f_samp)\n",
    "            window_psd[:, i] = np.resize(Pxx[1:], n_bins)\n",
    "            \n",
    "            # Calculate log10 of PSD, replacing points where PSD = 0 with 0 to avoid division by 0\n",
    "            for j in range(len(window_psd[:, i])):\n",
    "                if (window_psd[j, i] == 0):\n",
    "                    window_psd_log[j, i] = 0\n",
    "                else:\n",
    "                    window_psd_log[j, i] = np.log10(window_psd[j, i])\n",
    "            \n",
    "        # Append freq column\n",
    "        freq_col = np.transpose([np.resize(freq[:-1], n_bins)])\n",
    "        window_psd = np.append(window_psd, freq_col, axis=1)\n",
    "        window_psd_log = np.append(window_psd_log, freq_col, axis=1)\n",
    "        \n",
    "        dataset_psd.append(window_psd)\n",
    "        dataset_psd_log.append(window_psd_log)\n",
    "        \n",
    "    datasets_psd.update({label + '_PSD': dataset_psd})\n",
    "    datasets_psd_log.update({label +'_PSDLog': dataset_psd_log})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check again if its constructed correctly\n",
    "print('Num PSD\\'d windowed datasets: {}'.format(len(datasets_psd)))\n",
    "print('Num of PSD\\'d windows in first dataset: {}'.format(len(datasets_psd[dataset_labels[0]+'_PSD'])))\n",
    "print('Shape of PSD\\'d individual window: {}'.format(datasets_psd[dataset_labels[0]+'_PSD'][0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare some Log PSDs\n",
    "psds_to_plot = {label: dataset for label, dataset in datasets_psd_log.items() if 'Middle' in label and 'F8' in label and 'Kevin' in label}\n",
    "plot_set_transforms(psds_to_plot, win_num=0, dirn=4, transform_name='PSD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare some Log PSDs\n",
    "plot_all(np_to_pd(psds_to_plot, windowed=True), x_axis='Frequency', y_axis='Y Gyro', windowed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Structure:\n",
    "\n",
    "`Labelled Terrain Dataset\n",
    "-> Time Window\n",
    "---> 2D NumPy Array\n",
    "-----> Row = Transform Value for Frequency\n",
    "-----> Col = Dirn | Frequency`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) - 3D Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plot the set transforms in 3d'''\n",
    "def plot_transforms_3d(datasets_transformed, win_num, dirn, transform_name):\n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    \n",
    "    # Specific plot parameters for different transforms\n",
    "    if transform_name == 'FFT':\n",
    "        z_lim = (0, 1.6)\n",
    "        ax.set_zlim(z_lim[0], z_lim[1])\n",
    "        ax.set_zlabel('Amplitude (Normalized to Window Size)')\n",
    "    elif transform_name == 'PSD':\n",
    "        z_lim = (-10, -30)\n",
    "        ax.set_zlim(z_lim[0], z_lim[1])\n",
    "        ax.set_zlabel('Amplitude (Log-Scaled)')\n",
    "    \n",
    "    n_datasets = len(datasets_transformed)\n",
    "    \n",
    "    # Change y axis to corespond with terrain labels\n",
    "    ax.set_ylim(0.5, n_datasets+0.5)\n",
    "    subset = np.arange(0, n_datasets, dtype=int)\n",
    "    ax.set_yticks(np.add(subset, 1.5))\n",
    "    y_labels = [get_terrain_name(get_terrain_num(key)) for key in datasets_transformed.keys()]\n",
    "    ax.set_yticklabels(y_labels)\n",
    "    \n",
    "    ax.set_xlabel('Frequency ($Hz$)')\n",
    "    \n",
    "    # Bar graph paremeters\n",
    "    width = depth = 0.75\n",
    "    \n",
    "    # TODO: Sort by max amplitude\n",
    "    # Plot each dataset FFT\n",
    "    for i, (label, dataset) in enumerate(datasets_transformed.items()):\n",
    "        # To plot\n",
    "        x = dataset[win_num][:, -1]\n",
    "        y = [i+1]\n",
    "        top = dataset[win_num][:, dirn]\n",
    "        \n",
    "        # Specific to transform\n",
    "        bot = np.full(top.shape, z_lim[0])\n",
    "        ax.bar3d(x, y, bot, width, depth, top, alpha=0.8)\n",
    "        \n",
    "        # TODO: Make this nicer\n",
    "        ax.set_title(transform_name + ' of ' + get_columns(label)[dirn] + ', Window {}'.format(win_num))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare some FFTs in 3D\n",
    "ffts_to_plot = {label: dataset for label, dataset in datasets_fft.items() if 'Middle' in label and 'F8' in label and 'Kevin' in label}\n",
    "plot_transforms_3d(ffts_to_plot, win_num=4, dirn=2, transform_name='FFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare some PSDs in 3D\n",
    "psds_to_plot = {label: dataset for label, dataset in datasets_psd_log.items() if 'Middle' in label and 'F8' in label and 'Kevin' in label}\n",
    "plot_transforms_3d(psds_to_plot, win_num=4, dirn=4, transform_name='PSD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d) - Spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plot a spectogram of data'''\n",
    "def plot_spectogram(set_label, dirn, size):    \n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get sampling frequency for label\n",
    "    f_samp, f_low, f_high = get_frequencies(set_label)\n",
    "    \n",
    "    # Compute spectogram directly using time series data\n",
    "    plt.specgram(datasets[set_label][:, dirn], NFFT=size, Fs=f_samp)\n",
    "    plt.title('Spectrogram of {} for {}'.format(get_columns(set_label)[dirn], set_label))\n",
    "    plt.xlabel('Data Window')\n",
    "    plt.xticks(label=np.arange(0, len(datasets_windowed[set_label])))\n",
    "    plt.ylabel('Frequency ($Hz$)')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectogram(dataset_labels[0], dirn=1, size=get_window_size(dataset_labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract relevant features (e.g. Mean, Min, Skew, ...) from each data window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction functions\n",
    "\n",
    "# For small float values\n",
    "EPSILON = 0.00001\n",
    "\n",
    "'''L2 norm of an array'''\n",
    "def l2norm(array):\n",
    "    return np.linalg.norm(array, ord=2)\n",
    "\n",
    "'''Correlation of an array with itself'''\n",
    "def autocorr(array):\n",
    "    return np.correlate(array, array)[0]\n",
    "\n",
    "'''Root mean squared of an array'''\n",
    "def rms(array):\n",
    "    return np.sqrt(np.mean(array ** 2))\n",
    "\n",
    "'''Zero crossing rate of an array as a fraction of total size of array'''\n",
    "def zcr(array):\n",
    "    # Find locations where array > 0, put -1 and 1 for each rising/falling point from 0,\n",
    "    # divide by total datapoints in window\n",
    "    return len(np.nonzero(np.diff(array > 0.0))[0]) / len(array)\n",
    "\n",
    "'''Mean square frequency'''\n",
    "def msf(freqs, psd_amps):\n",
    "    num = np.sum(np.multiply(np.resize(freqs, len(psd_amps)), np.power(psd_amps, 2)))\n",
    "    denom = np.sum(psd_amps)\n",
    "    \n",
    "    # In case zero amplitude transform is ecountered\n",
    "    if denom <= EPSILON:\n",
    "        return EPSILON\n",
    "    \n",
    "    return np.divide(num, denom)\n",
    "\n",
    "'''Root mean square frequency'''\n",
    "def rmsf(freqs, psd_amps):\n",
    "    return np.sqrt(msf(freqs, psd_amps))\n",
    "\n",
    "'''Frequency center'''\n",
    "def fc(freqs, psd_amps):\n",
    "    num = np.sum(np.multiply(np.resize(freqs, len(psd_amps)), psd_amps))\n",
    "    denom = np.sum(psd_amps)\n",
    "    \n",
    "    # In case zero amplitude transform is ecountered\n",
    "    if denom <= EPSILON:\n",
    "        return EPSILON\n",
    "    \n",
    "    return np.divide(num, denom)\n",
    "\n",
    "'''Variance frequency'''\n",
    "def vf(freqs, psd_amps):\n",
    "    return msf(freqs, psd_amps) - fc(freqs, psd_amps) ** 2\n",
    "\n",
    "'''Root variance frequency'''\n",
    "def rvf(freqs, psd_amps):\n",
    "    return np.sqrt(msf(freqs, psd_amps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Extract given features from column of each dataset\n",
    "   Converts a dictionary of datasets to a nested dictionary where each dataset has its own dictionary\n",
    "   of axes/directions'''\n",
    "def feature_all(features, datasets, regular=True):\n",
    "    datasets_feat = {}\n",
    "    \n",
    "    # Calculate features for each window of each column of each dataset\n",
    "    for label, dataset in datasets.items():\n",
    "        directions = {}\n",
    "        n_data_col = get_n_data_col(label)\n",
    "        \n",
    "        # Loop over data columns\n",
    "        for i, direction in enumerate(get_columns(label)[:n_data_col]):\n",
    "            feats = {}\n",
    "            \n",
    "            if (regular):\n",
    "                '''Execute a function over all windows'''\n",
    "                def function_all_windows(function):\n",
    "                    feat_in_window = []\n",
    "                    \n",
    "                    for window in dataset:\n",
    "                        feat_in_window.append(function(window[:, i]))\n",
    "                    \n",
    "                    return feat_in_window        \n",
    "            else:\n",
    "                '''Alternate defintion for frequency functions'''\n",
    "                def function_all_windows(function):\n",
    "                    feat_in_window = []\n",
    "                    \n",
    "                    for window in dataset:\n",
    "                        feat_in_window.append(function(window[:, -1], window[:, i]))\n",
    "                    \n",
    "                    return feat_in_window\n",
    "                    \n",
    "            # Execute every function over all windows    \n",
    "            for feat_name, feat_func in features.items():\n",
    "                feats.update({feat_name: function_all_windows(feat_func)})\n",
    "            \n",
    "            directions.update({direction: pd.DataFrame.from_dict(feats)})\n",
    "\n",
    "        datasets_feat.update({label.replace('_PSD', ''): directions})\n",
    "    \n",
    "    return datasets_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plot a feature on all terrains for each time window'''\n",
    "def plot_set_features(datasets_feat, dirn, feat_name, placement_name, ylim=None, xlim=None):\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(10,8))\n",
    "    \n",
    "    for label, dataset in datasets_feat.items():\n",
    "        plt.plot(dataset[dirn][feat_name], label=label)\n",
    "    \n",
    "    # Set limits if they've been given\n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim[0], ylim[1])\n",
    "    if xlim is not None:\n",
    "        plt.xlim(xlim[0], xlim[1])\n",
    "    \n",
    "    plt.ylabel(feat_name)\n",
    "    plt.xlabel('Window #')\n",
    "    plt.title(placement_name + ' ' + dirn)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) - Time Domain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time domain feature functions and names\n",
    "time_features = {'Mean': np.mean, 'Std': np.std,  'Norm': l2norm, 'AC': autocorr, \n",
    "                 'Max': np.amax, 'Min' : np.amin, 'RMS': rms, 'ZCR': zcr, \n",
    "                 'Skew': stats.skew, 'EK': stats.kurtosis} \n",
    "\n",
    "# Create array of features of each window for each dataset and direction\n",
    "datasets_feat_time = feature_all(time_features, datasets_windowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if feature data is constructed correctly and print some info\n",
    "print('Num datasets: {}'.format(len(datasets_feat_time)))\n",
    "print('Num directions: {}'.format(len(datasets_feat_time[dataset_labels[0]])))\n",
    "print('Shape of first dataset first direction: {}'.format(datasets_feat_time[dataset_labels[0]]['X Accel'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some time feature data\n",
    "feat_datasets_to_plot = {label: dataset for label, dataset in datasets_feat_time.items() if 'Synthesis' in label and 'F8' in label and 'Kevin' in label}\n",
    "plot_set_features(feat_datasets_to_plot, dirn='Calc X Vel', feat_name='Min', placement_name='Middle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) - Frequency Domain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency domain feature functions and names\n",
    "freq_features = {'MSF': msf, 'RMSF': rmsf, 'FC': fc, 'VF': vf, 'RVF': rvf}\n",
    "\n",
    "# Create array of features of each window for each dataset and direction\n",
    "datasets_feat_freq = feature_all(freq_features, datasets_psd, regular=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if feature data is constructed correctly and print some info\n",
    "print('Num datasets: {}'.format(len(datasets_feat_freq)))\n",
    "print('Num directions: {}'.format(len(datasets_feat_freq[dataset_labels[0]])))\n",
    "print('Shape of one direction: {}'.format(datasets_feat_freq[dataset_labels[0]]['X Accel'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some frequency feature data\n",
    "feat_datasets_to_plot = {label: feature for label, feature in datasets_feat_freq.items() if 'Synthesis' in label and 'F8' in label and 'Kevin' in label}\n",
    "plot_set_features(feat_datasets_to_plot, dirn='Calc X Vel', feat_name='FC', placement_name='Middle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Structure\n",
    "\n",
    "`Terrain Dataset Dictionary\n",
    "-> Direction Dictionary \n",
    "---> Pandas Array\n",
    "-----> Row = Window #\n",
    "-----> Col = Extracted Feature`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 - Columning, Combination, and Standardization of Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert transform data to Pandas\n",
    "datasets_fft = np_to_pd(datasets_fft, windowed=True)\n",
    "datasets_psd_log = np_to_pd(datasets_psd_log, windowed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) - Columning Data\n",
    "\n",
    "Combine IMU data from each direction into single dataframes with columns for each feature in each direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Append a tag to the end of every column name of a dataframe'''\n",
    "def append_all_columns(columns, append_tag):\n",
    "    new_columns = []\n",
    "    \n",
    "    for column in columns:\n",
    "        new_columns.append(column + ' ' + append_tag)\n",
    "    \n",
    "    return new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (i) - Extracted Featured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Combined directions (axes) of a featured dataset'''\n",
    "def combine_extracted_columns(datasets):\n",
    "    datasets_combined = {}\n",
    "    \n",
    "    for label, dataset in datasets.items():\n",
    "        # Get labels array of first column\n",
    "        df_combined = pd.DataFrame()\n",
    "        \n",
    "        # Append direction name to feature name and combine everything in one frame\n",
    "        for dirn_label, dirn_df in dataset.items():\n",
    "            df_copy = pd.DataFrame(dirn_df)\n",
    "            \n",
    "            # Add direction and placement tags\n",
    "            df_copy.columns = append_all_columns(dirn_df.columns, dirn_label)\n",
    "            df_copy.columns = append_all_columns(dirn_df.columns, get_placement(label))\n",
    "            \n",
    "            df_combined = df_combined.join(dirn_df, how='outer')\n",
    "        \n",
    "        datasets_combined.update({label: df_combined})\n",
    "    \n",
    "    return datasets_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take time feature data and combine axes columns\n",
    "datasets_feat_time_columned = combine_extracted_columns(datasets_feat_time)\n",
    "\n",
    "# Confirm formatting\n",
    "datasets_feat_time_columned[dataset_labels[0]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take frequency feature data and axes columns\n",
    "datasets_feat_freq_columned = combine_extracted_columns(datasets_feat_freq)\n",
    "\n",
    "# Confirm formatting\n",
    "datasets_feat_freq_columned[dataset_labels[0]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (ii) - Transformed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Combined direction (axes) columns for transformed data'''\n",
    "def combine_transform_columns(datasets):\n",
    "    combined_datasets = {}\n",
    "    \n",
    "    for label, dataset in datasets.items():\n",
    "        # Get frequency bins from frequency column of first window\n",
    "        freq_bins = dataset[0]['Frequency'].tolist()\n",
    "        \n",
    "        # Get more parameter for current label\n",
    "        data_cols = get_columns(label)[:get_n_data_col(label)]\n",
    "        trans = get_transform(label)\n",
    "        place = get_placement(label)\n",
    "        \n",
    "        # Combine parameters to form columns for new combined DataFrame\n",
    "        new_cols = [trans + ' {} Hz '.format(round(f_bin)) + d_col + ' ' + place for d_col in data_cols for f_bin in freq_bins]\n",
    "        \n",
    "        # Convert windowed arrays into a single array with each window as a row\n",
    "        new_data = []\n",
    "        \n",
    "        for window in dataset:\n",
    "            new_row = []\n",
    "            for d_col in data_cols:\n",
    "                new_row.extend(window[d_col].tolist())\n",
    "            new_data.append(new_row)\n",
    "            \n",
    "        # Create new DataFrame\n",
    "        combined_df = pd.DataFrame(data=new_data, columns=new_cols)\n",
    "        combined_datasets.update({label: combined_df})\n",
    "\n",
    "    return combined_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets_fft_columned = combine_transform_columns(datasets_fft)\n",
    "\n",
    "# Confirm FFT formatting\n",
    "datasets_fft_columned[dataset_labels[0] + '_FFT'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets_psd_log_columned = combine_transform_columns(datasets_psd_log)\n",
    "\n",
    "# Check PSD formatting\n",
    "datasets_psd_log_columned[dataset_labels[0] + '_PSDLog'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) - Adding Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column containg the an integer label for each terrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Add labels to a dataset'\n",
    "def insert_labels(datasets):\n",
    "    # Returns new datasets\n",
    "    datasets_copy = {}\n",
    "    \n",
    "    # Add to each dataframe of a dataset\n",
    "    for label, dataset in datasets.items():\n",
    "        terrain_num = get_terrain_num(label)\n",
    "        labels = [terrain_num for _ in range(len(dataset))]\n",
    "        \n",
    "        dataset_copy = dataset.copy()\n",
    "        dataset_copy.insert(0, 'Label', labels)\n",
    "        \n",
    "        datasets_copy.update({label: dataset_copy})\n",
    "    \n",
    "    return datasets_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to each of the feature vector types\n",
    "datasets_feat_time_columned = insert_labels(datasets_feat_time_columned)\n",
    "datasets_feat_freq_columned = insert_labels(datasets_feat_freq_columned)\n",
    "datasets_fft_columned = insert_labels(datasets_fft_columned)\n",
    "datasets_psd_log_columned = insert_labels(datasets_psd_log_columned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check labelled data\n",
    "datasets_feat_time_columned[dataset_labels[0]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) - Combining Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data from each dataset into rows of a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Combine data from labelled datasets into a single dataframe'''\n",
    "def combine_datasets(datasets):\n",
    "    return pd.concat(list(datasets.values()), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Separate data according to placement, then combine data for each placement and return\n",
    "   as a dictionary of placements'''\n",
    "def separate_and_combine(datasets_columned):\n",
    "    datasets_separated = {}\n",
    "    \n",
    "    # Separate according to mounting location\n",
    "    for placement in placements:\n",
    "        datasets = {label: dataset for label, dataset in datasets_columned.items() if placement in label}\n",
    "        \n",
    "        # Update if not empty\n",
    "        if bool(datasets): \n",
    "            datasets_separated.update({placement: datasets})\n",
    "    \n",
    "    # Combine each mounting into a single combined dataframe and drop NaN values, export to csv\n",
    "    for placement, datasets in datasets_separated.items():\n",
    "        # TODO: Figure out where NaNs come from\n",
    "        # Seems to be Calc X Accel\n",
    "        datasets_separated.update({placement: combine_datasets(datasets).dropna(axis='columns')})\n",
    "                        \n",
    "    # Return dictionary of each mounting location\n",
    "    return datasets_separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each feature vector, combine datasets in two single dataframes\n",
    "time_feats_unnorm = separate_and_combine(datasets_feat_time_columned)\n",
    "freq_feats_unnorm = separate_and_combine(datasets_feat_freq_columned)                                      \n",
    "ffts_unnorm = separate_and_combine(datasets_fft_columned)\n",
    "psd_logs_unnorm = separate_and_combine(datasets_psd_log_columned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unnormalized data\n",
    "time_feats_unnorm['Synthesis'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d) - Standardization (aka Normalization)\n",
    "\n",
    "Standardize each feature to mean 0 and standard deviation 1. This makes feature selection and classification easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Normalize combined datasets\n",
    "   Returns normalized combined datasets, and the parameters (mean, standard dev) used for normalization\n",
    "   of each feature vector column'''\n",
    "def normalize_datasets(placements):\n",
    "    # Object to keep track of scaling parameters\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    placements_norm = {}\n",
    "    placements_norm_parameters = {}\n",
    "    \n",
    "    # Go through combined data\n",
    "    for placement_name, placement_combined_data in placements.items():\n",
    "        combined_data = placement_combined_data.copy()\n",
    "        \n",
    "        # Pop labels and add back in later, also keep track of columns\n",
    "        df_labels = combined_data.pop('Label').astype('int32')\n",
    "        df_cols = combined_data.columns\n",
    "        scaler = StandardScaler().fit(combined_data)\n",
    "        \n",
    "        # Keep track of mean and std deviation (scale_) of scaler for each column as a dictionary\n",
    "        placement_columns = {}\n",
    "        for i, df_col in enumerate(df_cols):\n",
    "            placement_columns.update({df_col: {'Mean': scaler.mean_[i], 'Scale': scaler.scale_[i]}})\n",
    "            \n",
    "        placements_norm_parameters.update({placement_name: placement_columns})\n",
    "        \n",
    "        # Normalize each combined feature and update placement dictionary\n",
    "        # Scaler converts to numpy so convert back to DataFrame\n",
    "        norm_combined_data = scaler.transform(combined_data)\n",
    "        norm_df = pd.DataFrame(data=norm_combined_data, columns=df_cols)\n",
    "        norm_df = norm_df.dropna(axis='rows')\n",
    "        norm_df.insert(0, 'Label', df_labels)\n",
    "        placements_norm.update({placement_name: norm_df})\n",
    "        \n",
    "    return placements_norm, placements_norm_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_feats, time_feat_params = normalize_datasets(time_feats_unnorm)\n",
    "freq_feats, freq_feat_params = normalize_datasets(freq_feats_unnorm)\n",
    "ffts, fft_params = normalize_datasets(ffts_unnorm)\n",
    "psd_logs, psd_log_params = normalize_datasets(psd_logs_unnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check combined and normalized extracted time feature vectors\n",
    "print('Time Feat Param Dictionary: {}'.format(time_feat_params['Synthesis']['Mean Calc X Vel Synthesis']))\n",
    "time_feats['Synthesis'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check combined and normalized extracted time feature vectors\n",
    "print('Freq Feat Param Dictionary: {}'.format(freq_feat_params['Synthesis']['FC Calc X Vel Synthesis']))\n",
    "freq_feats['Synthesis'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: More rows of exracted feature data are lost than transform features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check combined and normalized extracted time feature vectors\n",
    "print('FFT Parameter Dictionary: {}'.format(fft_params['Synthesis']['FFT 0 Hz Calc X Vel Synthesis']))\n",
    "ffts['Synthesis'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check combined and normalized extracted time feature vectors\n",
    "print('PSD Log Parameter Dictionary: {}'.format(psd_log_params['Synthesis']['PSDLog 0 Hz Calc X Vel Synthesis']))\n",
    "psd_logs['Synthesis'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7 - Exporting Data\n",
    "\n",
    "### Part (a) - Convert to .csv Files\n",
    "Convert feature vectors to .csv so we can use them in separate notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dict = {'TimeFeats': time_feats, 'FreqFeats': freq_feats, 'FFTs': ffts, 'PSDLogs': psd_logs}\n",
    "\n",
    "# Save each vector and each placement to .csv file\n",
    "for vector_name, vector_data in vector_dict.items():\n",
    "    for placement_name, placement_df in vector_data.items():\n",
    "        placement_df.to_csv('processed_data/' + placement_name + '_' + vector_name + '_' + 'All.csv',\n",
    "                            index=False)\n",
    "        \n",
    "# TODO: Use a pickle instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) - Pickle Normalization Parameter Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "relative_path = '/home/caris/Wheelchair-Terrain-Classification/dicts/'\n",
    "vector_param_dict = {'TimeFeats': time_feat_params, 'FreqFeats': freq_feat_params,\n",
    "                     'FFTs': fft_params, 'PSDLogs': psd_log_params}\n",
    "\n",
    "# Pickle each feature vector\n",
    "for vector_name, vector_param_dict in vector_param_dict.items():\n",
    "    norm_param_dict_filename = relative_path + vector_name + '_Norm_Param_Dictionary.pkl'\n",
    "    outfile = open(norm_param_dict_filename, 'wb')\n",
    "    pickle.dump(vector_param_dict, outfile)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To standardize data using already computed normalization parameters:\n",
    "\n",
    "`for each column of feature vector:\n",
    "    for each data point in column:\n",
    "        norm_value = (value - mean) / standard_deviation` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glossary\n",
    "\n",
    "`Dataset` - Batch of data recorded on one terrain type\n",
    "\n",
    "`Data Window` - Split up portion of a `Dataset`\n",
    "\n",
    "`Direction / Axes` - Linear acceleration or gyroscope in $x,y$ or $z$\n",
    "\n",
    "`Feature Vector` - Any feature of the data that can be used to classify terrain, e.g. Z Accel Mean, Y Accel FFT, etc\n",
    "\n",
    "`Extracted Feature Vector` - Features that aren't from transforms, e.g. Z Accel Min, Y Accel Autocorrelation, etc\n",
    "\n",
    "`Placement` - One of three IMU placements on the wheelchair, i.e. Middle, Left, or Right"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
