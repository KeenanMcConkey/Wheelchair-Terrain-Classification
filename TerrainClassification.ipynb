{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terrain Classification\n",
    "### Created by Keenan McConkey 2019.5.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "import sklearn.preprocessing as pre\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Importing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) - Parsing Data into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imu_data/PhoneLeftConcrete.csv',\n",
       " 'imu_data/PhoneLeftCarpet.csv',\n",
       " 'imu_data/WheelLeftConcrete.csv',\n",
       " 'imu_data/WheelLeftCarpet.csv',\n",
       " 'imu_data/WheelRightCarpet.csv',\n",
       " 'imu_data/PhoneMiddleAsphaltTest.csv',\n",
       " 'imu_data/PhoneMiddleConcrete.csv',\n",
       " 'imu_data/WheelLeftLinoleum.csv',\n",
       " 'imu_data/WheelRightConcrete.csv',\n",
       " 'imu_data/PhoneMiddleCarpet.csv',\n",
       " 'imu_data/PhoneRightCarpet.csv',\n",
       " 'imu_data/WheelRightLinoleum.csv',\n",
       " 'imu_data/PhoneMiddleLinoleum.csv',\n",
       " 'imu_data/FrameMiddleCarpet.csv',\n",
       " 'imu_data/FrameMiddleLinoleum.csv',\n",
       " 'imu_data/PhoneLeftLinoleum.csv',\n",
       " 'imu_data/PhoneRightConcrete.csv',\n",
       " 'imu_data/PhoneRightLinoleum.csv',\n",
       " 'imu_data/FrameMiddleConcrete.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare for import\n",
    "raw_datasets = {}\n",
    "dataset_paths = glob.glob('imu_data/*.csv')\n",
    "N_DATASETS = len(dataset_paths)\n",
    "\n",
    "dataset_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame_columns = ['YYYY-MO-DD HH-MI-SS_SSS', \n",
    "                 'ACCELEROMETER X (m/s²)', 'ACCELEROMETER Y (m/s²)', 'ACCELEROMETER Z (m/s²)', \n",
    "                 'GYROSCOPE X (rad/s)', 'GYROSCOPE Y (rad/s)', 'GYROSCOPE Z (rad/s)']\n",
    "\n",
    "wheel_columns = ['YYYY-MO-DD HH-MI-SS_SSS', 'Time received in s',\n",
    "                 'ACCELEROMETER X (m/s²)', 'ACCELEROMETER Y (m/s²)', 'ACCELEROMETER Z (m/s²)', \n",
    "                 'GYROSCOPE X (rad/s)', 'GYROSCOPE Y (rad/s)', 'GYROSCOPE Z (rad/s)']\n",
    "\n",
    "phone_columns = ['ACCELEROMETER X (m/s²)', 'ACCELEROMETER Y (m/s²)', 'ACCELEROMETER Z (m/s²)', \n",
    "                 'GYROSCOPE X (rad/s)', 'GYROSCOPE Y (rad/s)', 'GYROSCOPE Z (rad/s)',\n",
    "                 'Time since start in ms ', 'YYYY-MO-DD HH-MI-SS_SSS']\n",
    "renamed_columns = ['X Accel', 'Y Accel', 'Z Accel', 'X Gyro', 'Y Gyro', 'Z Gyro', 'Time']\n",
    "\n",
    "def get_columns(label):\n",
    "    if ('Phone' in label):\n",
    "        return phone_columns\n",
    "    elif ('Frame' in label):\n",
    "        return frame_columns\n",
    "    elif ('Wheel' in label):\n",
    "        return wheel_columns\n",
    "    else:\n",
    "        raise Exception('Unknown label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import datasets as an array of Pandas DataFrames\n",
    "TRIM_LEN = 2000 # Number of data points to trim from each side\n",
    "N_DATA_COL = 6 # Number of columns containing directional data\n",
    "\n",
    "remove_columns = ['LINEAR ACCELERATION X (m/s²)', 'LINEAR ACCELERATION Y (m/s²)', 'LINEAR ACCELERATION Z (m/s²)',\n",
    "                  'GRAVITY X (m/s²)', 'GRAVITY Y (m/s²)', 'GRAVITY Z (m/s²)', \n",
    "                  'MAGNETIC FIELD X (μT)', 'MAGNETIC FIELD Y (μT)', 'MAGNETIC FIELD Z (μT)',\n",
    "                  'ORIENTATION X (pitch °)', 'ORIENTATION Y (roll °)', 'ORIENTATION Z (azimuth °)']\n",
    "dataset_labels = []\n",
    "\n",
    "for dataset_path in dataset_paths:\n",
    "    # Parse labels\n",
    "    dataset_name = dataset_path.split(' ')[-1].split('/')[-1].split('.')[0]\n",
    "    dataset_labels.append(dataset_name)\n",
    "    \n",
    "    # Read from CSV\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "        \n",
    "    # Drop unused columns\n",
    "    for to_remove in remove_columns:\n",
    "        if (to_remove in dataset.columns):\n",
    "            dataset = dataset.drop(to_remove, axis=1)\n",
    "            #print('Removed ' + to_remove)\n",
    "            \n",
    "    # Convert string format to epoch time\n",
    "    if ('Phone' in dataset_name):\n",
    "        dataset['YYYY-MO-DD HH-MI-SS_SSS'] = dataset['YYYY-MO-DD HH-MI-SS_SSS'].apply(time.strptime, \n",
    "                                                                                      args=(\"%Y-%m-%d %H:%M:%S:%f\",))\n",
    "        dataset['YYYY-MO-DD HH-MI-SS_SSS'] = dataset['YYYY-MO-DD HH-MI-SS_SSS'].apply(time.mktime)\n",
    "    \n",
    "    # Trim edges to account for startup time\n",
    "    #dataset = dataset[TRIM_LEN:-TRIM_LEN]\n",
    "    \n",
    "    raw_datasets.update({dataset_name: dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_columns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3a37958ed5f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcorrect_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-3a37958ed5f0>\u001b[0m in \u001b[0;36mcorrect_columns\u001b[0;34m(dataset, label)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcorrect_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcorrect_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_columns' is not defined"
     ]
    }
   ],
   "source": [
    "def correct_columns(dataset, label):\n",
    "    dataset.columns = get_columns(label)\n",
    "\n",
    "for label, dataset in raw_datasets.items():\n",
    "    correct_columns(dataset, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print some useful information\n",
    "print('Labels: {}'.format(dataset_labels))\n",
    "raw_datasets['FrameMiddleCarpet'].head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the update rate of the IMU is non deterministic and lower than the rate the phone samples it at, i.e. the phone receives a non-deterministic number of sequential identical measurements from the IMU when polling at approx 200 Hz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) - Visualizing Time Domain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Compare phones mounted in different locations'''\n",
    "def phone_compare(dirn, surface):\n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(30, 10))\n",
    "    \n",
    "    plt.plot(raw_datasets['PhoneLeft' + surface]['YYYY-MO-DD HH-MI-SS_SSS'],\n",
    "             raw_datasets['PhoneLeft' + surface][dirn], label='Left')\n",
    "    plt.plot(raw_datasets['PhoneRight' + surface]['YYYY-MO-DD HH-MI-SS_SSS'],\n",
    "             raw_datasets['PhoneRight' + surface][dirn], label='Right')\n",
    "    plt.plot(raw_datasets['PhoneMiddle' + surface]['YYYY-MO-DD HH-MI-SS_SSS'],\n",
    "             raw_datasets['PhoneMiddle' + surface][dirn], label='Middle')\n",
    "    \n",
    "    plt.xlabel('Epoch Time (s)')\n",
    "    plt.ylabel(dirn + ' on ' + surface)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phone_compare('ACCELEROMETER X (m/s²)', 'Concrete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Compare different labels'''\n",
    "def label_compare(label1, label2, dirn, t_offset=0, y_offset=0):\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(figsize=(30,10))\n",
    "    \n",
    "    # Plot with offsets\n",
    "    ax.plot(raw_datasets[label1]['YYYY-MO-DD HH-MI-SS_SSS'].apply(lambda x: x + t_offset), \n",
    "             raw_datasets[label1][dirn].apply(lambda x: x + y_offset), label=label1)\n",
    "    ax.plot(raw_datasets[label2]['YYYY-MO-DD HH-MI-SS_SSS'], raw_datasets[label2][dirn], label=label2)\n",
    "    \n",
    "    ax.set_xlabel('Epoch Time ($s$)')\n",
    "    ax.set_ylabel(dirn)\n",
    "    ax.set_title(dirn + ' on ' + label1 + ' and ' + label2)\n",
    "    offset_text = 'Offsets\\n'\n",
    "    offset_text += label1 + ': t={}'.format(t_offset) + ', ' + 'y={}'.format(y_offset)\n",
    "    ax.text(0.05, 0.05, s=offset_text, \n",
    "            horizontalalignment='left', verticalalignment='bottom', transform=ax.transAxes)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Corrected\n",
    "label_compare('PhoneMiddleLinoleum', 'FrameMiddleLinoleum', 'ACCELEROMETER Z (m/s²)', t_offset=-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Corrected\n",
    "label_compare('PhoneMiddleLinoleum', 'FrameMiddleLinoleum', 'GYROSCOPE Z (rad/s)', t_offset=-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Corrected\n",
    "label_compare('PhoneRightLinoleum', 'WheelRightLinoleum', 'ACCELEROMETER Z (m/s²)', t_offset=-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uncorrected\n",
    "label_compare('WheelLeftLinoleum', 'WheelRightLinoleum', 'GYROSCOPE Z (rad/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uncorrected\n",
    "label_compare('WheelLeftLinoleum', 'WheelRightLinoleum', 'ACCELEROMETER Z (m/s²)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uncorrected\n",
    "label_compare('FrameMiddleConcrete', 'WheelRightConcrete', 'ACCELEROMETER Z (m/s²)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uncorrected\n",
    "label_compare('FrameMiddleCarpet', 'WheelRightCarpet', 'GYROSCOPE Z (rad/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Plot given x annd y axes for every dataset in given array of datasets'''\n",
    "def plot_all(_datasets, x_axis, y_axis, x_unit='($ms$)', y_unit='($m/s^2$)'):\n",
    "    plt.clf()\n",
    "    \n",
    "    n_axes = len(_datasets)\n",
    "    odd_axes = n_axes % 2 == 1\n",
    "    \n",
    "    rows = int((n_axes + 1) / 2)\n",
    "    \n",
    "    # Scale approriately\n",
    "    if (odd_axes):\n",
    "        fig = plt.figure(figsize=(n_axes*5, n_axes*3))          \n",
    "    else:\n",
    "        fig = plt.figure(figsize=(n_axes*5, n_axes*2))\n",
    "    \n",
    "    # Grid of subplots\n",
    "    gs = gridspec.GridSpec(rows, 2)\n",
    "        \n",
    "    axes = []\n",
    "    row, col = 0, 0\n",
    "    \n",
    "    for i, (label, dataset) in enumerate(_datasets.items()):\n",
    "        # Take a whole row if odd num of axes\n",
    "        if (i == n_axes-1 and odd_axes): \n",
    "            axes.append(fig.add_subplot(gs[row, :]))\n",
    "        else:\n",
    "            axes.append(fig.add_subplot(gs[row, col]))\n",
    "        \n",
    "        # Plot on new subplot\n",
    "        axes[i].plot(dataset[x_axis], dataset[y_axis])\n",
    "        axes[i].set_title(label)\n",
    "        axes[i].set_xlabel(x_axis + ' ' + x_unit)\n",
    "        axes[i].set_ylabel(y_axis + ' ' + y_unit)\n",
    "        \n",
    "        # Only go two columns wide\n",
    "        col += 1\n",
    "        if (col == 2):\n",
    "            row += 1\n",
    "            col = 0\n",
    "        \n",
    "    plt.subplots_adjust(hspace=0.35, wspace=0.15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot some examples\n",
    "datasets_to_plot = {}\n",
    "\n",
    "# Only frame data\n",
    "for label, dataset in raw_datasets.items():\n",
    "    if ('Frame' in label):\n",
    "        datasets_to_plot.update({label: dataset})\n",
    "        \n",
    "plot_all(datasets_to_plot,\n",
    "         x_axis='YYYY-MO-DD HH-MI-SS_SSS', \n",
    "         y_axis='ACCELEROMETER Z (m/s²)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) - Converting Between Pandas and Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Convert array of Pandas DataFrames to array of 2D Numpy array'''\n",
    "def pd_to_np(pd_datasets, windowed=False):\n",
    "    np_datasets = {}\n",
    "    \n",
    "    for label, dataset in pd_datasets.items():\n",
    "        np_dataset = []\n",
    "        \n",
    "        # If windowed, convert individual windows to Pandas\n",
    "        if (windowed):\n",
    "            for window in dataset:\n",
    "                np_dataset.append(window.as_matrix()) \n",
    "        else:\n",
    "            np_dataset = dataset.as_matrix()\n",
    "        \n",
    "        np_datasets.update({label: np_dataset})\n",
    "        \n",
    "    return np_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Convert array of 2D Numpy arrays to Pandas Data Frames'''\n",
    "'''Need to add columns names later'''\n",
    "def np_to_pd(np_datasets, windowed=False):\n",
    "    pd_datasets = {}\n",
    "    \n",
    "    for label, dataset in np_datasets.items():\n",
    "        pd_dataset = []\n",
    "        \n",
    "        # Use correct column names\n",
    "        new_columns = get_columns(label)\n",
    "            \n",
    "        # If windowed, convert individual windows to Pandas\n",
    "        if (windowed):\n",
    "            for window in dataset:\n",
    "                pd_dataset.append(pd.DataFrame(data=window, columns=new_columns))\n",
    "        else:\n",
    "            pd_dataset = pd.DataFrame(data=dataset, columns=new_columns)\n",
    "            \n",
    "        pd_datasets.update({label: pd_dataset})\n",
    "    \n",
    "    return pd_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to NumPy\n",
    "raw_datasets = pd_to_np(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if its constructed correctly\n",
    "print('Num datasets: {}'.format(len(raw_datasets)))\n",
    "print('Shape of first dataset: {}'.format(raw_datasets[dataset_labels[0]].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Filtering\n",
    "\n",
    "Datasheets of smartphone level IMUs suggest that IMU filters data before sending it to the phone, and that the cutoff frequency of this filtering is configurable and changes with update frequency.\n",
    "\n",
    "Based on the IMU update rate of about 20 ms, this cutoff frequency is already close to 40 Hz, so filtering is probably unnecessary. Its hard to be sure about actual values because this is configured by the phone manufacturer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) - Butterworth Filtering\n",
    "\n",
    "Butterworth filters can be high-pass/low-pass/bandpass, and attempt to have maximally flat frequency response in bandpass.\n",
    "\n",
    "Changing the *cutoff frequency* of the filter affects the smoothness of the graph and amount of ringing. \n",
    "\n",
    "Changing the *order* of the filter can have significant effects on smoothness depending on the cutoff frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datasets = {} # Filtered data \n",
    "\n",
    "F_SAMP = 200 # Sampling frequency\n",
    "F_CUT = 40 # Cutoff frequency\n",
    "\n",
    "w = F_CUT / (F_SAMP / 2) # Normalize the frequency\n",
    "\n",
    "# Get Butterworth filter parameters\n",
    "b_butter, a_butter = signal.butter(N=4, Wn=w, btype='low')\n",
    "\n",
    "# Filter each data column of every dataset\n",
    "for label, dataset in raw_datasets.items(): # Make a copy first\n",
    "    datasets.update({label: np.copy(dataset)})\n",
    "\n",
    "for label, dataset in datasets.items():\n",
    "    for i in range(N_DATA_COL):\n",
    "        dataset[:, i] = signal.filtfilt(b_butter, a_butter, dataset[:,i])\n",
    "\n",
    "# Compare filtered and unfiltered\n",
    "plt.clf()\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(datasets['FrameMiddleCarpet'][:, 1], label='Filtered')\n",
    "plt.plot(raw_datasets['FrameMiddleCarpet'][:, 1], label='Unfiltered')\n",
    "plt.xlim(0, 200)\n",
    "plt.legend()\n",
    "plt.xlabel('Datapoint')\n",
    "plt.ylabel(' Z Accel ($m/s^2$)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check construction\n",
    "print('Num filtered datasets: {}'.format(len(datasets)))\n",
    "print('Shape of first filtered dataset: {}'.format(datasets[dataset_labels[0]].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verify we can convert back to Pandas\n",
    "# TODO: Fix this crashing when run multiple times\n",
    "np_to_pd(datasets, windowed=False)[dataset_labels[0]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot all filtered datasets\n",
    "plot_all(np_to_pd(datasets), x_axis='YYYY-MO-DD HH-MI-SS_SSS', y_axis='ACCELEROMETER Z (m/s²)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Time Windows\n",
    "\n",
    "### Part (a) - Finding an optimal time window\n",
    "\n",
    "Strategy is to start with a large time window and work down. \n",
    "\n",
    "Plot classification accuracy and time vs window size and find an optimal size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) - Creating Time Windowed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Create an array of different window sizes, taking out the desired array so don't have to rename things\n",
    "\n",
    "WINDOW_SIZE = 800 # Divide by 200 to get sample size in seconds\n",
    "datasets_windowed = {}\n",
    "\n",
    "# Trim excess datapoints, then split into windows\n",
    "for label, dataset in datasets.items():   \n",
    "    n_windows = int(len(dataset) / WINDOW_SIZE)\n",
    "    n_points = n_windows*WINDOW_SIZE\n",
    "    \n",
    "    dataset_windowed = np.resize(dataset, (n_points, dataset.shape[1]))\n",
    "    dataset_windowed = np.split(dataset_windowed, n_windows, axis=0)\n",
    "    \n",
    "    datasets_windowed.update({label: dataset_windowed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if its constructed correctly\n",
    "print('Num windowed datasets: {}'.format(len(datasets_windowed)))\n",
    "print('Num of windows in first dataset: {}'.format(len(datasets_windowed[dataset_labels[0]])))\n",
    "print('Shape of individual window: {}'.format(datasets_windowed[dataset_labels[0]][0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of the data right now:\n",
    "\n",
    "`1D Array containing each dataset -> 1D Array containg each window -> 2D NP Array of actual data for each sensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Try out pandas conversion again\n",
    "np_to_pd(datasets_windowed, windowed=True)[dataset_labels[0]][0].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - FFT and PSD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Plot tranform of given direction and window'''\n",
    "def plot_set_transforms(datasets_transformed, dirn, window, label):\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for label, dataset in datasets_transformed.items():\n",
    "        plt.plot(dataset[window][:, -1], dataset[window][:, dirn], \n",
    "                 label=label)\n",
    "\n",
    "    plt.xlabel('Frequency ($Hz$)')\n",
    "    if (label == 'FFT'):\n",
    "        plt.ylabel('Amplitude (Normalized to Window Size)')\n",
    "    elif (label == 'PSD'):\n",
    "        plt.ylabel('Amplitude (Log-Scaled)')\n",
    "    \n",
    "    plt.title(label +' of ' + get_columns(label)[dirn] + \n",
    "              ', Window {}, Window Size = {} Data Points'.format(window, WINDOW_SIZE))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) - FFT\n",
    "Its possible the FFT is not valid due to the non determinisitic update rate. Probably needs some interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets_fft = {}\n",
    "\n",
    "# Find the FFT of each column of each data window of each dataset\n",
    "for label, dataset in datasets_windowed.items():\n",
    "    dataset_fft = []\n",
    "    \n",
    "    for window in dataset:\n",
    "        # Number of frequency bins is half of window size to trim the symmetric higher frequencies\n",
    "        n_bins = int(WINDOW_SIZE / 2)\n",
    "        \n",
    "        # Trim and add a new column for frequency\n",
    "        window_fft = np.resize(window, (n_bins, window.shape[1]+1))\n",
    "        \n",
    "        for i in range(N_DATA_COL):\n",
    "            # FFT is normalized to window size, to ensure consistency between datasets\n",
    "            window_fft[:, i] = np.resize(np.abs(np.divide(np.fft.fft(window[:, i], axis=0), \n",
    "                                                          WINDOW_SIZE)), n_bins)\n",
    "            \n",
    "        freq_col = np.transpose([np.linspace(0.0, F_SAMP / 2, n_bins)])\n",
    "        # Append the frequency column\n",
    "        np.append(window_fft, freq_col, axis=1)\n",
    "        \n",
    "        dataset_fft.append(window_fft)\n",
    "        \n",
    "    datasets_fft.update({label: dataset_fft})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check again if its constructed correctly\n",
    "print('Num of FFT\\'d windowed datasets: {}'.format(len(datasets_fft)))\n",
    "print('Num of FFT\\'d windows in first dataset: {}'.format(len(datasets_fft[dataset_labels[0]])))\n",
    "print('Shape of FFT\\'d individual window: {}'.format(datasets_fft[dataset_labels[0]][0].shape))\n",
    "print('{}'.format(datasets_fft[dataset_labels[0]][0][:, 8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot Z Accel of 3rd window\n",
    "plot_set_transforms(datasets_fft, 6, 0, 'FFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Butterworth cutoff frequency is pretty noticeable. Theres not an easily recognizable cutoff from the unfiltered data.\n",
    "\n",
    "**The number of frequency bins is dependent on window sizem and it seems to affect amplitude too.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try Numpy conversion again\n",
    "np_to_pd(datasets_fft, windowed=True, new_col='Frequency')[0][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) - PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets_psd = []\n",
    "datasets_psd_log = []\n",
    "\n",
    "# Find the PSD of each column of each data window of each dataset\n",
    "for dataset in datasets_windowed:\n",
    "    dataset_psd = []\n",
    "    dataset_psd_log = []\n",
    "    \n",
    "    for window in dataset:\n",
    "        # Add a new column for freq\n",
    "        window_psd = np.resize(window, (int(WINDOW_SIZE / 2), window.shape[1] + 1))\n",
    "        window_psd_log = np.copy(window_psd)\n",
    "        \n",
    "        for i in range(N_DATA_COL):\n",
    "            # Normalized PSD - Returns frequencies and power density\n",
    "            freq, Pxx = signal.periodogram(window[:, i], F_SAMP)\n",
    "            window_psd[:, i] = np.resize(Pxx[1:], int(WINDOW_SIZE / 2))\n",
    "            window_psd_log[:, i] = np.log10(window_psd[:, i])\n",
    "            \n",
    "        # Append freq column\n",
    "        window_psd[:, N_DATA_COL + 1] = freq[:-1]\n",
    "        window_psd_log[:, N_DATA_COL + 1] = freq[:-1]\n",
    "        \n",
    "        dataset_psd.append(window_psd)\n",
    "        dataset_psd_log.append(window_psd_log)\n",
    "        \n",
    "    datasets_psd.append(dataset_psd)\n",
    "    datasets_psd_log.append(dataset_psd_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check again if its constructed correctly\n",
    "print('Num PSD\\'d windowed datasets: {}'.format(len(datasets_psd)))\n",
    "print('Num of PSD\\'d windows in first dataset: {}'.format(len(datasets_psd[0])))\n",
    "print('Shape of PSD\\'d individual window: {}'.format(datasets_psd[0][0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Z Accel of 3rd window\n",
    "plot_set_transforms(datasets_psd_log, 2, 3, 'PSD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that at this point, data is stored like:\n",
    "\n",
    "`Labelled Terrain Dataset -> Time Window -> 2D NumPy Array (6 cols FFT/PSD in one direction, col 7 is time, col 8 is freq)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) - 3D Visualization\n",
    "\n",
    " - FFT/PSD in 3D for each terrain\n",
    " - FFT/PSD in 3D for each direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Plot the set transforms in 3d'''\n",
    "def plot_transforms_3d(datasets_trans, win, label, to_plot, set_num=0, dirn=0, start=0, stop=N_DATASETS):\n",
    "    subset = np.arange(start, stop, dtype=int)\n",
    "    \n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    \n",
    "    # Specific to FFT or PSD\n",
    "    if (label == 'FFT'):\n",
    "        ax.set_zlim(0, 1.5)\n",
    "        ax.set_zlabel('Amplitude (Normalized)')\n",
    "    elif (label == 'PSD'):\n",
    "        ax.set_zlim(-5, 2)\n",
    "        ax.set_zlabel('Amplitude (Log-Scaled)')\n",
    "    \n",
    "    ax.set_ylim(start, stop)\n",
    "    ax.set_yticks(np.add(subset, 1))\n",
    "    if (to_plot == 'datasets'):\n",
    "        ax.set_yticklabels(dataset_labels)\n",
    "    elif (to_plot == 'directions'):\n",
    "        ax.set_yticklabels(dataset_columns)\n",
    "    ax.set_xlabel('Frequency ($Hz$)')\n",
    "    \n",
    "    ax.set_title(label + ' of ' + dataset_columns[dirn] + \n",
    "                 ', Window {}, Window Size = {} Data Points'.format(win, WINDOW_SIZE))\n",
    "        \n",
    "    # Plot each dataset FFT\n",
    "    for i in subset:\n",
    "        if (to_plot == 'datasets'):\n",
    "            ax.plot(xs=datasets_trans[i][win][:, N_DATA_COL+1], ys=datasets_trans[i][win][:, dirn], \n",
    "                zs=i+1, zdir='y', label=dataset_labels[i])\n",
    "\n",
    "        elif (to_plot == 'directions'):\n",
    "            ax.plot(xs=datasets_trans[set_num][win][:, N_DATA_COL+1], ys=datasets_trans[set_num][win][:, i], \n",
    "                zs=i+1, zdir='y', label=dataset_columns[i])\n",
    "            \n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot 3d Z Accel FFT of 2nd window\n",
    "plot_transforms_3d(datasets_fft, 0, 'FFT', to_plot='datasets', dirn=0, stop=N_DATASETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot 3d Z Accel PSD of 2nd window\n",
    "plot_transforms_3d(datasets_psd_log, 0, 'PSD', to_plot='datasets', dirn=0, stop=N_DATASETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot 3d Z Accel FFT of 2nd window\n",
    "plot_transforms_3d(datasets_fft, 0, 'FFT', to_plot='directions', stop=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d) - Spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Plot a spectogram of data'''\n",
    "def plot_spectogram(set_num, dirn, size=WINDOW_SIZE):\n",
    "    # Compute spectogram directly using time series data\n",
    "    # Match size of each data point bin to window size\n",
    "    plt.specgram(datasets[set_num][:, dirn], NFFT=size, Fs=F_SAMP)\n",
    "    plt.title('Spectrogram of {} for {}'.format(dataset_columns[dirn], dataset_labels[set_num]))\n",
    "    plt.xlabel('Data Window')\n",
    "    plt.xticks(label=np.arange(0, len(datasets_windowed[set_num])))\n",
    "    plt.ylabel('Frequency ($Hz$)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X Accel\n",
    "plot_spectogram(2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Feature Extraction\n",
    "\n",
    "Data storage of features:\n",
    "\n",
    "`Labelled Terrain Dataset -> Direction -> Array with elements extracted from each time window, row = window #, col = feature`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature extraction functions\n",
    "\n",
    "'''L2 norm of an array'''\n",
    "def l2norm(array):\n",
    "    return np.linalg.norm(array, ord=2)\n",
    "\n",
    "'''Correlation of an array with itself'''\n",
    "def autocorr(array):\n",
    "    return np.correlate(array, array)\n",
    "\n",
    "'''Root mean squared of an array'''\n",
    "def rms(array):\n",
    "    return np.sqrt(np.mean(array ** 2))\n",
    "\n",
    "'''Zero crossing rate of an array as a fraction of total size of array'''\n",
    "def zcr(array):\n",
    "    # Locations where array > 0, put -1 and 1 for rising/falling,\n",
    "    # divide by total datapoints\n",
    "    return len(np.nonzero(np.diff(array > 0))[0]) / len(array)\n",
    "\n",
    "'''Mean square frequency'''\n",
    "def msf(freqs, psd_amps):\n",
    "    num = np.sum(np.multiply(np.resize(freq, len(psd_amps)), np.power(psd_amps, 2)))\n",
    "    denom = np.sum(psd_amps)\n",
    "    return num / denom\n",
    "\n",
    "'''Root mean square frequency'''\n",
    "def rmsf(freqs, psd_amps):\n",
    "    return np.sqrt(msf(freqs, psd_amps))\n",
    "\n",
    "'''Frequency center'''\n",
    "def fc(freqs, psd_amps):\n",
    "    num = np.sum(np.multiply(np.resize(freq, len(psd_amps)), psd_amps))\n",
    "    denom = np.sum(psd_amps)\n",
    "    return num / denom\n",
    "\n",
    "'''Variance frequency'''\n",
    "def vf(freqs, psd_amps):\n",
    "    return msf(freqs, psd_amps) - fc(freqs, psd_amps) ** 2\n",
    "\n",
    "'''Root variance frequency'''\n",
    "def rvf(freqs, psd_amps):\n",
    "    return np.sqrt(msf(freqs, psd_amps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Extract given features from everything in dataset'''\n",
    "def feature_all(feat_funcs, feat_names, datasets, regular=True):\n",
    "    datasets_feat = []\n",
    "    \n",
    "    # Calculate features for each window of each column of each dataset\n",
    "    for dataset in datasets:\n",
    "        directions = []\n",
    "\n",
    "        for i in range(N_DATA_COL):\n",
    "            feats = []\n",
    "            \n",
    "            if (regular):\n",
    "                '''Execute a function over all windows'''\n",
    "                def function_all_windows(function):\n",
    "                    feat_in_window = []\n",
    "                    \n",
    "                    for window in dataset:\n",
    "                        feat_in_window.append(function(window[:, i]))\n",
    "                    \n",
    "                    return feat_in_window\n",
    "                    \n",
    "            else:\n",
    "                '''Alternate defintion for frequency functions'''\n",
    "                def function_all_windows(function):\n",
    "                    feat_in_window = []\n",
    "                    \n",
    "                    for window in dataset:\n",
    "                        feat_in_window.append(function(window[:, N_DATA_COL+1], window[:, i]))\n",
    "                    \n",
    "                    return feat_in_window\n",
    "                    \n",
    "                    \n",
    "            # Execute every function over all windows    \n",
    "            for func in feat_funcs:\n",
    "                feats.append(function_all_windows(func))\n",
    "            \n",
    "            directions.append(np.transpose(np.array(feats)))\n",
    "\n",
    "        datasets_feat.append(directions)\n",
    "    \n",
    "    return datasets_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Plot a feature on all terrains for each time window'''\n",
    "def plot_set_features(datasets_feat, feat_names, dirn=0, feat=0):\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    for i in range(N_DATASETS):\n",
    "        plt.plot(datasets_feat[i][dirn][:, feat], label=dataset_labels[i])\n",
    "        \n",
    "    plt.ylabel(feat_names[feat])\n",
    "    plt.xlabel('Window #')\n",
    "    plt.title(dataset_columns[dirn] + ', Window Size = {} Data Points'.format(WINDOW_SIZE))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) - Time Domain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Time domain feature function names and actual names\n",
    "time_feat_funcs = [np.mean, np.std, l2norm, autocorr, np.amax, np.amin, rms, zcr, stats.skew, stats.kurtosis]\n",
    "time_feat_names = ['Mean', 'Std Dev', 'L2 Norm', 'Autocorrelation', 'Max', 'Min', 'Root Mean Squared',\n",
    "                   'Zero Crossing Rate', 'Skew', 'Excess Kurtosis']\n",
    "N_TIME_FEATS = len(time_feat_funcs)\n",
    "\n",
    "# Create array of features of each window for each dataset and direction\n",
    "datasets_feat_time = feature_all(time_feat_funcs, time_feat_names, datasets_windowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if its constructed correctly and print some info\n",
    "print('Num datasets: {}'.format(len(datasets_feat_time)))\n",
    "print('Num directions: {}'.format(len(datasets_feat_time[0])))\n",
    "print('Shape of first dataset first direction: {}'.format(datasets_feat_time[0][0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Max of Z Accel\n",
    "plot_set_features(datasets_feat_time, time_feat_names, dirn=2, feat=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) - Frequency Domain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Time domain feature function names and actual names\n",
    "freq_feat_funcs = [msf, rmsf, fc, vf, rvf]\n",
    "freq_feat_names = ['Mean Square Frequency', 'Root Mean Square Frequency', 'Frequency Center', 'Variance Frequency',\n",
    "                   'Root Variance Frequency']\n",
    "N_FREQ_FEATS = len(freq_feat_names)\n",
    "\n",
    "# Calculate features for each window of each column of each dataset\n",
    "# Create array of features of each window for each dataset and direction\n",
    "datasets_feat_freq = feature_all(freq_feat_funcs, freq_feat_names, datasets_psd, regular=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if its constructed correctly and print some info\n",
    "print('Num datasets: {}'.format(len(datasets_feat_freq)))\n",
    "print('Num directions: {}'.format(len(datasets_feat_freq[0])))\n",
    "print('Shape of one direction: {}'.format(datasets_feat_freq[0][0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot RVF of Z Accel\n",
    "plot_set_features(datasets_feat_freq, freq_feat_names, dirn=2, feat=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of very similar shapes i.e. highly correlated variables in both time and frequency. Need to implement feature selection.\n",
    "\n",
    "**Ideally we can implement feature selection over all directions, with FFT + PSD + Time Features + Freq Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verify pandas conversion\n",
    "np_to_pd(datasets_feat_freq, windowed=True)[0][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'Add labels to a dataset'\n",
    "def insert_labels(datasets):\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for j, direction in enumerate(dataset):\n",
    "            # Insert terrain labels\n",
    "            labels = [dataset_labels[i] for k in range(len(direction))]\n",
    "            direction.insert(0, 'Label', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) - Standardization\n",
    "\n",
    "Standardize each feature to mean 0 and standard deviation 1. This will make PCA and classification easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Normalize an already featured dataset'''\n",
    "def normalize_featured_datasets(datasets, n_feats):\n",
    "    for dataset in datasets:\n",
    "        for direction in dataset:\n",
    "            for i in range(n_feats):\n",
    "                direction[:, i] = pre.scale(direction[:, i])\n",
    "\n",
    "'''Normalize a transformed dataset'''\n",
    "def normalize_transformed_datasets(datasets):\n",
    "    for dataset in datasets:\n",
    "        for window in dataset:\n",
    "            for i in range(N_DATA_COL):\n",
    "                window[:, i] = pre.scale(window[:, i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalize_featured_datasets(datasets_feat_freq, N_FREQ_FEATS)\n",
    "normalize_featured_datasets(datasets_feat_time, N_TIME_FEATS)\n",
    "normalize_transformed_datasets(datasets_fft)\n",
    "normalize_transformed_datasets(datasets_psd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verify that each feature has mean 0 and variance one, here for Grass X Accel time\n",
    "for i in range(N_TIME_FEATS):\n",
    "    feat = datasets_feat_time[0][0][:, i]\n",
    "    print('Mean = {}, Var = {}'.format(np.mean(feat), np.var(feat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot normalized features\n",
    "plot_set_features(datasets_feat_time, time_feat_names, dirn=2, feat=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot normalized fft\n",
    "plot_set_transforms(datasets_fft, 2, 3, 'FFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) - mRMR\n",
    "\n",
    "Necessary because of the number of different features/transforms in each direction.\n",
    "\n",
    "Try to find which features are most relevant, from all directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets_feat_freq = np_to_pd(datasets_feat_freq, windowed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Insert terrain labels\n",
    "insert_labels(datasets_feat_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets_feat_freq[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Need to convert everything to a single pandas dataset\n",
    "def to_single_dataframe(datasets, feat_names):\n",
    "    # Create the columns of the array\n",
    "    \n",
    "    columns = ['Label']\n",
    "    for j in range(N_DATA_COL):\n",
    "        for name in feat_names:\n",
    "            columns.append(dataset_columns[j] + ' ' + name)\n",
    "    dataframe = pd.DataFrame(columns=columns, index=[])\n",
    "    \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        set_dataframe = pd.DataFrame(columns)\n",
    "        \n",
    "        for j, direction in enumerate(dataset[1:]):\n",
    "            # Direction labelled columns\n",
    "            direction_columns = ['Label']\n",
    "            for name in feat_names:\n",
    "                direction_columns.append(dataset_columns[j] + ' ' + name)\n",
    "            \n",
    "            # Data directions into a single dataframe\n",
    "            direction.columns = direction_columns\n",
    "            set_dataframe = set_dataframe.append(direction)\n",
    "            \n",
    "        # Combine datasets into a single dataframe\n",
    "        dataframe.append(dataset)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_feats = to_single_dataframe(datasets_feat_freq, freq_feat_names)\n",
    "freq_feats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glossary\n",
    "\n",
    "`Dataset` - Batch of data recorded on one terrain type\n",
    "\n",
    "`Data Window` - Split up portion of a `Dataset`\n",
    "\n",
    "`Direction` - Linear acceleration or gyroscope in $x,y$ or $z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
